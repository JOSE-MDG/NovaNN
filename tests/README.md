# Testing en NovaNN v2.0.0

## üß™ Visi√≥n General

NovaNN incluye una suite completa de tests unitarios que verifica la correcta implementaci√≥n de todos los componentes del framework. Con una cobertura >95%, los tests aseguran que cada capa, optimizador, funci√≥n de p√©rdida y utilidad funcione correctamente tanto en forward como en backward pass.

## üìÅ Estructura de Tests

```
üìÅ tests
‚îú‚îÄ‚îÄ üìÅ initializers
‚îÇ   ‚îî‚îÄ‚îÄ üêç test_init.py
‚îú‚îÄ‚îÄ üìÅ layers
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ activations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_leaky_relu.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_relu.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_sigmoid.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_softmax.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_tanh.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ batch_norm
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_batchnorm1d.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_batchnorm2d.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ conv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_conv1d.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_conv2d.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ linear
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_linear.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ pooling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ gap
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_gap1d.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_gap2d.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ maxpool
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üêç test_maxpooling1d.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üêç test_maxpooling2d.py
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ regularization
‚îÇ       ‚îî‚îÄ‚îÄ üêç test_dropout.py
‚îú‚îÄ‚îÄ üìÅ optimizers
‚îÇ   ‚îú‚îÄ‚îÄ üêç test_adam.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç test_rmsprop.py
‚îÇ   ‚îî‚îÄ‚îÄ üêç test_sgd.py
‚îî‚îÄ‚îÄ üìÅ sequential
    ‚îî‚îÄ‚îÄ üêç test_sequential.py
```

### `üìÇ tests/`

**Suite de tests unitarios para verificar la correcta implementaci√≥n de todos los componentes de NovaNN**

Contiene tests organizados por m√≥dulos que verifican funcionalidad, gradientes y comportamiento en diferentes modos de todas las capas, optimizadores, inicializadores y utilidades del framework.

#### `üìÇ tests/üìÇ dataloader/`

##### `test_dataloader.py`

- **Prop√≥sito**: Verifica el comportamiento correcto del `DataLoader`, especialmente el manejo del √∫ltimo batch cuando el tama√±o del dataset no es m√∫ltiplo del batch size
- **Pruebas principales**:
  - `test_last_batch_size()`: Asegura que el √∫ltimo batch tenga el tama√±o correcto (2 muestras cuando batch_size=4 y dataset de 10 muestros)
- **Metodolog√≠a**:
  - Crea dataset sint√©tico de 10 muestras con batch_size=4
  - Verifica que se produzcan 3 batches (4, 4, 2 muestras)
  - Confirma que el √∫ltimo batch tenga exactamente 2 muestras

#### `üìÇ tests/üìÇ initializers/`

##### `test_init.py`

- **Prop√≥sito**: Verifica las funciones de inicializaci√≥n de pesos (`kaiming_normal_`, `kaiming_uniform_`, `xavier_normal_`, `xavier_uniform_`, `random_init_`)
- **Pruebas principales**:
  - `test_kaiming_normal_distribution()`: Verifica media ‚âà0 y desviaci√≥n est√°ndar correcta para Kaiming normal
  - `test_kaiming_uniform_distribution()`: Verifica que valores est√©n dentro de l√≠mites uniformes calculados
  - `test_xavier_normal_distribution()`: Verifica media ‚âà0 y varianza correcta para Xavier normal
  - `test_xavier_uniform_distribution()`: Verifica l√≠mites uniformes para Xavier uniform
  - `test_random_initializer()`: Verifica media ‚âà0 para inicializaci√≥n aleatoria peque√±a
  - `test_exceptions_of_init_methods()`: Verifica que nonlinearities no soportadas levanten `ValueError`
- **Metodolog√≠a**:
  - Prueba con m√∫ltiples formas tensoriales (2D a 5D)
  - Compara estad√≠sticas muestrales con valores te√≥ricos esperados
  - Usa `calculate_gain` y `shape_validation` de `novann.core.init`
  - Tolerancias emp√≠ricas (0.1) para estad√≠sticas

#### `üìÇ tests/üìÇ layers/üìÇ activations/`

**Tests para verificar la correcta implementaci√≥n de las funciones de activaci√≥n**

##### `test_relu.py`

- **Prop√≥sito**: Verifica la capa `ReLU` (Rectified Linear Unit)
- **Pruebas**:
  - `test_relu_forward_backward_and_numeric()`: Comprueba forward (no negatividad), backward (m√°scara de gradiente) y gradiente num√©rico para entradas no cero
- **Metodolog√≠a**:
  - Forward: Verifica forma y propiedad `max(0, x)`
  - Backward: Compara con m√°scara `(x > 0)`
  - Gradiente num√©rico: Usa `numeric_grad_elementwise` para validar gradientes anal√≠ticos
  - Excluye `x = 0` donde la derivada no est√° definida

##### `test_leaky_relu.py`

- **Prop√≥sito**: Verifica la capa `LeakyReLU` con pendiente negativa configurable
- **Pruebas**:
  - `test_leaky_relu_forward_backward_and_numeric()`: Comprueba forward (comportamiento piecewise), backward (gradiente piecewise) y gradiente num√©rico
- **Metodolog√≠a**:
  - Forward: Verifica `x` si `x ‚â• 0`, `slope * x` si `x < 0`
  - Backward: Compara con `1` (x ‚â• 0) y `slope` (x < 0)
  - Gradiente num√©rico: Validaci√≥n con diferencias finitas para entradas no cero

##### `test_sigmoid.py`

- **Prop√≥sito**: Verifica la capa `Sigmoid`
- **Pruebas**:
  - `test_sigmoid_forward_backward_and_numeric()`: Comprueba forward (rango (0,1)), backward (gradiente) y gradiente num√©rico
- **Metodolog√≠a**:
  - Forward: Verifica forma y rango `0 < œÉ(x) < 1`
  - Backward: Compara con f√≥rmula anal√≠tica `œÉ(x) * (1 - œÉ(x))`
  - Gradiente num√©rico: Validaci√≥n completa con `numeric_grad_elementwise`

##### `test_softmax.py`

- **Prop√≥sito**: Verifica la capa `Softmax` con estabilidad num√©rica y propiedades de probabilidad
- **Pruebas**:
  - `test_softmax_forward_properties_and_shift_invariance_columnwise()`: Verifica propiedades forward (suma a 1 por fila, no negatividad, invariancia a desplazamiento)
  - `test_softmax_backward_numeric_columnwise()`: Verifica backward usando producto Jacobiano-vector y gradiente num√©rico
- **Metodolog√≠a**:
  - Forward: Suma a 1, no negatividad, invariancia a constante aditiva
  - Backward: Compara gradiente anal√≠tico con aproximaci√≥n num√©rica usando `numeric_grad_scalar_from_softmax`

##### `test_tanh.py`

- **Prop√≥sito**: Verifica la capa `Tanh` (tangente hiperb√≥lica)
- **Pruebas**:
  - `test_tanh_forward_backward_and_numeric()`: Comprueba forward (rango (-1,1), propiedad de funci√≥n impar), backward (gradiente) y gradiente num√©rico
- **Metodolog√≠a**:
  - Forward: Verifica forma, rango `-1 < tanh(x) < 1` y propiedad `tanh(-x) = -tanh(x)`
  - Backward: Compara con f√≥rmula anal√≠tica `1 - tanh¬≤(x)`
  - Gradiente num√©rico: Validaci√≥n con `numeric_grad_elementwise`

#### `üìÇ tests/üìÇ layers/üìÇ batch_norm/`

**Tests para verificar las implementaciones de Batch Normalization en 1D y 2D**

##### `test_batchnorm1d.py`

- **Prop√≥sito**: Verifica la capa `BatchNorm1d` para normalizaci√≥n por lotes en entradas 1D/2D
- **Pruebas**:
  - `test_batchnorm1d_forward_train_mode()`: Verifica forward en modo entrenamiento (centrado y normalizaci√≥n por caracter√≠sticas, actualizaci√≥n de estad√≠sticas m√≥viles)
  - `test_batchnorm1d_forward_eval_mode()`: Verifica forward en modo evaluaci√≥n (uso de estad√≠sticas m√≥viles, sin actualizaci√≥n)
  - `test_batchnorm1d_backward_gradient_check()`: Verifica gradientes anal√≠ticos vs num√©ricos para par√°metros `gamma` y `beta`
  - `test_batchnorm1d_momentum_and_eps()`: Verifica par√°metros de momentum y √©psilon personalizados
  - `test_batchnorm1d_parameters()`: Verifica que el m√©todo `parameters()` retorne los par√°metros correctos
- **Metodolog√≠a**:
  - Modo entrenamiento: Verifica media ‚âà0 y varianza ‚âà1 por caracter√≠stica despu√©s de normalizaci√≥n
  - Modo evaluaci√≥n: Verifica uso de estad√≠sticas m√≥viles y estabilidad num√©rica
  - Gradientes: Usa `numeric_grad_wrt_param` para comparar gradientes anal√≠ticos y num√©ricos de `gamma` y `beta`
  - Par√°metros: Verifica formas de estad√≠sticas m√≥viles y listas de par√°metros

##### `test_batchnorm2d.py`

- **Prop√≥sito**: Verifica la capa `BatchNorm2d` para normalizaci√≥n por lotes en entradas 2D convolucionales (4D)
- **Pruebas**:
  - `test_batchnorm2d_forward_train_mode()`: Verifica forward en modo entrenamiento para datos 4D (normalizaci√≥n por canal sobre dimensiones espaciales)
  - `test_batchnorm2d_forward_eval_mode()`: Verifica forward en modo evaluaci√≥n con estad√≠sticas m√≥viles
  - `test_batchnorm2d_backward_gradient_check()`: Verifica gradientes de `gamma` y `beta` con gradientes num√©ricos
  - `test_batchnorm2d_momentum_and_eps()`: Verifica par√°metros de momentum y √©psilon
  - `test_batchnorm2d_different_spatial_sizes()`: Verifica comportamiento con diferentes tama√±os espaciales
  - `test_batchnorm2d_parameters()`: Verifica m√©todo `parameters()`
- **Metodolog√≠a**:
  - Modo entrenamiento: Verifica media ‚âà0 y varianza ‚âà1 por canal (reducci√≥n sobre ejes batch, height, width)
  - Modo evaluaci√≥n: Verifica uso de estad√≠sticas m√≥viles sin actualizaci√≥n
  - Gradientes: Compara gradientes anal√≠ticos de `gamma` y `beta` con aproximaciones num√©ricas
  - Tama√±os espaciales: Prueba con diferentes alturas y anchos, verificando conservaci√≥n de forma
  - Estad√≠sticas m√≥viles: Verifica formas `(1, C, 1, 1)` para broadcasting

#### `üìÇ tests/üìÇ layers/üìÇ conv/`

**Tests para capas convolucionales 1D y 2D**

##### `test_conv1d.py`

- **Prop√≥sito**: Verificar la capa `Conv1d` (convoluci√≥n 1D para procesamiento de secuencias)
- **Pruebas principales**:
  - `test_conv1d_forward_shape()`: Verifica la forma de salida en forward pass con diferentes configuraciones
  - `test_conv1d_forward_no_bias()`: Verifica forward sin t√©rmino de bias
  - `test_conv1d_backward_gradient_check()`: Verifica gradientes de pesos y bias mediante comparaci√≥n con gradientes num√©ricos
  - `test_conv1d_padding_modes()`: Prueba diferentes modos de padding (zeros, reflect, replicate, circular)
  - `test_conv1d_parameters()`: Verifica que el m√©todo `parameters()` retorne los par√°metros correctos
- **Metodolog√≠a**:
  - Usa RNG determin√≠stico para reproducibilidad
  - Calcula formas esperadas usando f√≥rmulas: $L_{out} = \lfloor\frac{L_{in} + 2 \times \text{padding} - K}{\text{stride}}\rfloor + 1$
  - Para verificaci√≥n de gradientes: compara gradientes anal√≠ticos (`layer.weight.grad`, `layer.bias.grad`) con aproximaciones num√©ricas usando `numeric_grad_wrt_param`
  - Tolerancia `THRESHOLD=5e-3` para diferencias m√°ximas

##### `test_conv2d.py`

- **Prop√≥sito**: Verificar la capa `Conv2d` (convoluci√≥n 2D para procesamiento de im√°genes)
- **Pruebas principales**:
  - `test_conv2d_forward_shape()`: Verifica formas de salida en 4D
  - `test_conv2d_forward_no_bias()`: Verifica forward sin bias
  - `test_conv2d_backward_gradient_check_small()`: Verifica gradientes con entradas peque√±as para eficiencia
  - `test_conv2d_different_kernel_stride_padding()`: Prueba combinaciones de kernel, stride y padding (incluyendo tuplas para dimensiones separadas)
  - `test_conv2d_padding_modes()`: Prueba diferentes modos de padding
  - `test_conv2d_parameters()`: Verifica m√©todo `parameters()`
- **Metodolog√≠a**:
  - Calcula dimensiones esperadas: $H_{out} = \lfloor\frac{H_{in} + 2 \times p_h - K_h}{s_h}\rfloor + 1$, similar para ancho
  - Verificaci√≥n de gradientes con entradas reducidas (`6x6`) para mantener tiempos de ejecuci√≥n manejables
  - Misma tolerancia `THRESHOLD=5e-3` para comparaciones
  - Soporte para configuraciones asim√©tricas (kernels, strides, paddings como tuplas)

#### `üìÇ tests/üìÇ layers/üìÇ linear/`

**Tests para capas lineales (fully connected)**

##### `test_linear.py`

- **Prop√≥sito**: Verificar la capa `Linear` (transformaci√≥n lineal completamente conectada)
- **Pruebas principales**:
  - `test_linear_forward_shape()`: Verifica forma de salida `(batch, out_features)`
  - `test_linear_forward_no_bias()`: Verifica forward sin t√©rmino de bias
  - `test_linear_backward_gradient_check()`: Verifica gradientes de pesos y bias con gradientes num√©ricos
- **Metodolog√≠a**:
  - Usa RNG determin√≠stico
  - Verifica formas y tipos de datos (`dtype=np.float32`)
  - Compara gradientes anal√≠ticos vs num√©ricos usando `numeric_grad_wrt_param` para ambos par√°metros (weight, bias)
  - Tolerancia `THRESHOLD=5e-3`

#### `üìÇ tests/üìÇ layers/üìÇ pooling/`

**Tests para capas de pooling (reducci√≥n dimensional)**

##### `tests/layers/pooling/gap/`

**Tests para Global Average Pooling**

##### `test_gap1d.py`

- **Prop√≥sito**: Verificar la capa `GlobalAvgPool1d` (pooling global promedio en 1D)
- **Pruebas principales**:
  - `test_global_avg_pool1d_forward_shape()`: Verifica que colapse dimensi√≥n de longitud a 1
  - `test_global_avg_pool1d_forward_values()`: Verifica c√°lculo correcto del promedio con valores constantes
  - `test_global_avg_pool1d_backward_gradient()`: Verifica gradiente con comparaci√≥n num√©rica
  - `test_global_avg_pool1d_uniform_gradient()`: Verifica distribuci√≥n uniforme del gradiente (cada elemento recibe $1/L$)
- **Metodolog√≠a**:
  - Forward: verifica forma `(batch, channels, 1)` y valores de promedio
  - Backward: usa `numeric_grad_scalar_wrt_x` para comparaci√≥n num√©rica
  - Distribuci√≥n uniforme: verifica que gradiente sea $1/L$ donde $L$ es la longitud original

##### `test_gap2d.py`

- **Prop√≥sito**: Verificar la capa `GlobalAvgPool2d` (pooling global promedio en 2D)
- **Pruebas principales**:
  - `test_global_avg_pool2d_forward_shape()`: Verifica colapso de dimensiones espaciales a `1x1`
  - `test_global_avg_pool2d_forward_values()`: Verifica c√°lculo de promedio con valores constantes
  - `test_global_avg_pool2d_backward_gradient()`: Verifica gradiente con comparaci√≥n num√©rica
  - `test_global_avg_pool2d_uniform_gradient()`: Verifica distribuci√≥n uniforme del gradiente (cada elemento recibe $1/(H \times W)$)
- **Metodolog√≠a**:
  - Similar a `test_gap1d.py` pero para 4D tensores
  - Verifica forma `(batch, channels, 1, 1)`
  - Distribuci√≥n uniforme sobre √°rea espacial

#### `üìÇ tests/üìÇ layers/üìÇ pooling/üìÇ maxpool/`

**Tests para Max Pooling**

##### `test_maxpooling1d.py`

- **Prop√≥sito**: Verificar la capa `MaxPool1d` (pooling m√°ximo en 1D)
- **Pruebas principales**:
  - `test_maxpool1d_forward_shape()`: Verifica forma de salida con kernel=2, stride=2
  - `test_maxpool1d_forward_padding()`: Verifica forma con padding
  - `test_maxpool1d_backward_gradient()`: Verifica gradiente con comparaci√≥n num√©rica
  - `test_maxpool1d_stride_different()`: Verifica con stride diferente al kernel
- **Metodolog√≠a**:
  - Calcula dimensiones esperadas usando f√≥rmula de convoluci√≥n
  - Backward: comparaci√≥n con `numeric_grad_scalar_wrt_x`
  - Tolerancia `THRESHOLD=5e-3`

##### `test_maxpooling2d.py`

- **Prop√≥sito**: Verificar la capa `MaxPool2d` (pooling m√°ximo en 2D)
- **Pruebas principales**:
  - `test_maxpool2d_forward_shape()`: Verifica forma de salida con kernel=2, stride=2
  - `test_maxpool2d_forward_padding()`: Verifica forma con padding
  - `test_maxpool2d_backward_gradient()`: Verifica gradiente con comparaci√≥n num√©rica
- **Metodolog√≠a**:
  - Similar a `test_maxpooling1d.py` pero para 2D
  - Verifica formas 4D
  - Misma tolerancia para comparaci√≥n de gradientes

#### `üìÇ tests/üìÇ layers/üìÇ regularization/`

**Tests para capas de regularizaci√≥n**

##### `test_dropout.py`

- **Prop√≥sito**: Verificar la capa `Dropout` (regularizaci√≥n por apagado aleatorio de neuronas)
- **Pruebas principales**:
  - `test_dropout_eval_mode()`: Verifica que en modo evaluaci√≥n no se aplique dropout (la entrada pasa sin cambios)
  - `test_dropout_train_mode()`: Verifica que en modo entrenamiento se aplique m√°scara aleatoria y escalado correcto
  - `test_dropout_zero_probability()`: Verifica que probabilidades inv√°lidas (p=0.0) levanten `ValueError`
- **Metodolog√≠a**:
  - Modo evaluaci√≥n: Comprueba que entrada y salida sean id√©nticas, y que los gradientes pasen sin cambios
  - Modo entrenamiento: Verifica que aproximadamente `(1-p)` fracci√≥n de elementos se conserven, que los valores conservados escalen por `1/(1-p)`, y que los gradientes se enmascaren y escalen de la misma manera
  - Validaci√≥n de par√°metros: Comprueba que solo se acepten probabilidades en el rango `(0, 1)`
- **Detalles**:
  - Usa tensores de prueba grandes (`100x100`) para obtener estad√≠sticas confiables
  - Tolerancia del 5% para variaci√≥n aleatoria en la proporci√≥n de elementos conservados
  - Verifica coherencia entre forward y backward (misma m√°scara y escalado)

#### `üìÇ tests/üìÇ optimizers/`

**Tests para optimizadores**

##### `test_adam.py`

- **Prop√≥sito**: Verificar el optimizador `Adam` (Adaptive Moment Estimation)
- **Pruebas principales**:
  - `test_adam_basic_update()`: Verifica que Adam actualice par√°metros de una capa `Linear`
  - `test_adam_with_conv_layer()`: Verifica que Adam funcione con capas convolucionales
  - `test_adam_bias_correction()`: Verifica el mecanismo de correcci√≥n de bias en pasos tempranos
- **Metodolog√≠a**:
  - Comprueba que los par√°metros cambien despu√©s de `step()`
  - Verifica que el contador de pasos (`t`) se incremente
  - Para la correcci√≥n de bias, ejecuta m√∫ltiples pasos y verifica que todas las actualizaciones sean no nulas
  - Usa capas reales (`Linear`, `Conv2d`) con forward/backward simulados
- **Integraci√≥n**: Depende de `Adam` de `novann/optim/` y de capas del framework

##### `test_adamw.py`

- **Prop√≥sito**: Verificar el optimizador `AdamW` (Adam con weight decay desacoplado)
- **Pruebas principales**:
  - `test_adamw_updates_parameters()`: Verifica que AdamW actualice par√°metros correctamente y que el contador de pasos (`t`) se incremente
  - `test_adamw_decoupled_weight_decay()`: Verifica que el weight decay se aplique **separadamente** de la actualizaci√≥n del gradiente (caracter√≠stica distintiva de AdamW vs Adam)
  - `test_adamw_excludes_batchnorm_from_weight_decay()`: Verifica que AdamW **no** aplique weight decay a par√°metros `gamma` y `beta` de BatchNorm
- **Metodolog√≠a**:
  - **Actualizaci√≥n b√°sica**: Genera gradientes sint√©ticos, ejecuta `step()` y verifica cambios en par√°metros
  - **Weight decay desacoplado**: Compara dos modelos id√©nticos (uno con `weight_decay=0.5`, otro con `weight_decay=0.0`) tras un paso de optimizaci√≥n. Verifica que la magnitud de actualizaci√≥n con decay sea **menor** que sin decay, confirmando el efecto de regularizaci√≥n desacoplada
  - **Exclusi√≥n de BatchNorm**: Crea un modelo con capas `Conv2d` (debe recibir decay) y `BatchNorm2d` (no debe recibir decay). Asigna nombres `"gamma"` y `"beta"` a los par√°metros de BatchNorm. Tras `step()`, verifica que:
    - Los pesos de Conv cambien (gradiente + weight decay)
    - Los par√°metros de BatchNorm cambien solo por el gradiente (sin amplificaci√≥n de decay)
- **Integraci√≥n**: Depende de `AdamW` de `novann/optim/` y de capas `Linear`, `Conv2d`, `BatchNorm2d` del framework

##### `test_rmsprop.py`

- **Prop√≥sito**: Verificar el optimizador `RMSprop` (Root Mean Square Propagation)
- **Pruebas principales**:
  - `test_rmsprop_basic_update()`: Verifica actualizaci√≥n b√°sica de par√°metros
  - `test_rmsprop_with_weight_decay()`: Verifica el efecto de weight decay (L2) en la magnitud de par√°metros
  - `test_rmsprop_zero_grad()`: Verifica que `zero_grad()` limpie los gradientes
- **Metodolog√≠a**:
  - Compara par√°metros antes y despu√©s de `step()` para confirmar actualizaci√≥n
  - Para weight decay: compara dos modelos id√©nticos (con y sin decay) tras un paso de optimizaci√≥n
  - Para `zero_grad()`: verifica que todos los gradientes se pongan a cero
- **Nota**: El test de weight decay actualmente verifica que las normas sean iguales (con tolerancia), lo cual podr√≠a refinarse para verificar que la norma con decay sea menor.

##### `test_sgd.py`

- **Prop√≥sito**: Verificar el optimizador `SGD` (Stochastic Gradient Descent) con momentum y gradient clipping
- **Pruebas principales**:
  - `test_sgd_basic_update()`: Verifica actualizaci√≥n b√°sica en un modelo `Sequential` con m√∫ltiples capas
  - `test_sgd_with_momentum()`: Verifica el efecto de momentum en actualizaciones consecutivas
  - `test_sgd_gradient_clipping()`: Verifica que los gradientes se recorten correctamente al `max_grad_norm` especificado mediante clipping global
  - `test_sgd_zero_grad()`: Verifica que `zero_grad()` limpie gradientes
- **Metodolog√≠a**:
  - Usa un modelo `Sequential` con dos capas `Linear` para prueba integral
  - Para momentum: ejecuta dos pasos con el mismo gradiente y verifica que el segundo paso tenga mayor magnitud (acumulaci√≥n de velocidad)
  - **Para gradient clipping**: Crea un gradiente artificialmente grande (`100.0` en todos los elementos), configura `max_grad_norm=1.0`, ejecuta `step()` y verifica que la norma L2 del gradiente resultante sea aproximadamente `1.0` (dentro de tolerancia `1e-5`), confirmando que el clipping global funcion√≥ correctamente
  - Para `zero_grad()`: verifica que gradientes existan antes y sean cero despu√©s
  
#### `üìÇ tests/üìÇ sequential/`

**Tests para el contenedor Sequential (apilado de capas)**

##### `test_sequential.py`

- **Prop√≥sito**: Verificar el contenedor `Sequential`, que permite apilar m√∫ltiples capas y ejecutarlas en secuencia, tanto en forward como en backward pass, incluyendo manejo de modos (train/eval) y utilidades de inicializaci√≥n.
- **Pruebas principales**:
  - `test_sequential_linear_activation()`: Verifica secuencias con capas lineales y funciones de activaci√≥n variadas (ReLU, LeakyReLU, Sigmoid, Tanh), comprobando formas de salida y rangos esperados.
  - `test_sequential_conv_pooling()`: Verifica secuencias con capas convolucionales (Conv1d, Conv2d) y de pooling (MaxPool, GlobalAvgPool) para procesamiento 1D y 2D.
  - `test_sequential_mixed_layers()`: Verifica secuencias complejas con mezcla de capas (Conv, Dropout, Flatten, Linear, Softmax) y comportamiento diferenciado en modos train vs eval.
  - `test_sequential_backward()`: Verifica la propagaci√≥n backward completa a trav√©s de m√∫ltiples capas, comprobando formas de gradientes y existencia de gradientes en todos los par√°metros.
  - `test_sequential_initialization_helpers()`: Verifica m√©todos internos `_find_next_activation` y `_find_last_activation` usados para inicializaci√≥n inteligente de pesos.
  - `test_sequential_parameters_and_zero_grad()`: Verifica que `parameters()` retorne todos los par√°metros de las capas contenidas y que `zero_grad()` limpie correctamente los gradientes.
- **Metodolog√≠a**:
  - Crea modelos `Sequential` con arquitecturas variadas (MLP, CNN).
  - En forward: pasa tensores de entrada sint√©ticos y verifica formas, rangos de salida y propiedades (ej. suma a 1 con Softmax).
  - En backward: calcula gradientes respecto a salidas aleatorias y verifica propagaci√≥n correcta a trav√©s de todas las capas.
  - Modos train/eval: alterna entre modos y verifica comportamientos espec√≠ficos (ej. Dropout activo solo en train).
  - Par√°metros: cuenta y verifica acceso a todos los `weight` y `bias` de capas internas.
  - Utilidades de inicializaci√≥n: simula b√∫squeda de funciones de activaci√≥n adyacentes a capas lineales para inicializaci√≥n adecuada (Kaiming/Xavier).