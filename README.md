![Banner](./images/NovaNN%20Banners.png)

![version](https://img.shields.io/badge/version-2.0.0-blue)
![python](https://img.shields.io/badge/python-v3.14-brightgreen)
![license](https://img.shields.io/badge/license-MIT-blue)
![tests](https://img.shields.io/badge/tests-pytest-orange)
![coverage](https://img.shields.io/badge/coverage-95%25-success)

## üåê Idiomas disponibles

- üá¨üáß [English](README.en.md)
- üá™üá∏ [Espa√±ol](README.md)

**NovaNN** es un framework **que** ofrece herramientas y ejemplos para la creaci√≥n de redes neuronales **Fully Connected** y **convolucionales** junto con m√≥dulos que brindan soporte y mejoran el entrenamiento de la red. Este proyecto **demuestra** una comprensi√≥n profunda y dominio sobre c√≥mo funcionan estas redes, inspirado en c√≥mo lo hacen los frameworks de deep learning m√°s populares como **PyTorch** y **TensorFlow**, especialmente **PyTorch**, que sirvi√≥ como inspiraci√≥n principal para este proyecto

**Aclaraci√≥n**: Este framework fue creado con fines educativos para tener un idea clara de que hacen los grandes frameworks de Deep Learning. **Objetivo**: Demostrar conocimientos s√≥lidos en: **redes neuronales**, **Deep Learning**, **Machine Learning**, **matem√°ticas**, **ingenier√≠a de software**, **Dise√±o de sistemas**, **buenas pr√°cticas**, **tests unitarios**, **dise√±o ultra-modular** y **preprocesamiento de datos**.

## Introducci√≥n

 **NovaNN** cuenta con una estructura completamente **modular dise√±ada** para que sea lo m√°s parecido a un framework

 El directorio `data/` est√° destinado a datasets como _Fashion-MNIST_ y _MNIST_. Dado que los archivos originales no se incluyen en el repositorio por su tama√±o, puedes descargarlos desde **Kaggle** mediante los siguientes enlaces:
  - [fasion-mnist-train](https://www.kaggle.com/datasets/zalando-research/fashionmnist?select=fashion-mnist_train.csv)
  - [fasion-mnist-test](https://www.kaggle.com/datasets/zalando-research/fashionmnist?select=fashion-mnist_test.csv)
  - [mnist-train](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv?select=mnist_train.csv)
  - [mnist-test](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv?select=mnist_test.csv)
 

 El directorio `examples/` contiene scripts de ejemplos como **clasificaci√≥n binaria**, **clasificaci√≥n multiclase**, **regresi√≥n** y **capas convolucionales**.

 En `notebooks/` encontrar√°s un cuaderno de Jupyter que prepara los datos de validaci√≥n a partir de los datasets descargados.
 **Nota importante**: Verifica la estructura de los datos antes de ejecutar el notebook, ya que variaciones en el formato pueden causar errores.

 Tambi√©n es **necesario crear un archivo `.env`** con las siguientes variables de entorno:

 - **FASHION_TRAIN_DATA_PATH**: Ruta de datos de entrenamiento
 - **EXPORTATION_FASHION_TRAIN_DATA_PATH**: Ruta de datos de entrenamiento separado de los datos de validaci√≥n.
 - **FASHION_VALIDATION_DATA_PATH**: Ruta de datos de validaci√≥n separados de los de entrenamiento. 
 - **FASHION_TEST_DATA_PATH**: Ruta de los datos de prueba

 - **MNIST_TRAIN_DATA_PATH**: Ruta de datos de entrenamiento
 - **EXPORTATION_MNIST_TRAIN_DATA_PATH**: Ruta de datos de entrenamiento separado de los datos de validaci√≥n.
 - **MNIST_VALIDATION_DATA_PATH**: Ruta de datos de validaci√≥n separados de los de entrenamiento. 
 - **MNIST_TEST_DATA_PATH**: Ruta de los datos de prueba

 - **LOG_FILE**: Ruta del archivo de logs
 - **LOGGER_DEFAULT_FORMAT**: `%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s - %(message)s` <- Valor por defecto. 
 - **LOGGER_DATE_FORMAT** `%Y-%m-%d %H:%M:%S` <- Valor por defecto.

 - **Comparaci√≥n con PyTorch**: Se evalu√≥ el rendimiento de **NovaNN** frente al framework **PyTorch** en una tarea de clasificaci√≥n con el dataset de _MNIST_, utilizando el mismo dataset e hiperpar√°metros en ambas implementaciones. Para obtener los resultados del cotejo se guard√≥ en formato `json` m√©tricas como el accuracy y la p√©rdida.

 - **[main.py](main.py)**: Este archivo implementa el c√≥digo de entrenamiento y la estructura de la red que se va a utilizar para el cotejo.
 - **[cloab](https://colab.research.google.com/drive/1M6Qo2vu4mjVJWQGBK6I4PFBvwwXbQvvj?usp=sharing)**: En el notebook est√° el c√≥digo de entrenamiento versi√≥n para PyTorch, que realiza el mismo procedimiento que el script.

### Resultados del cotejo:

Una vez obtenidos los resultados, se hizo un script ([visualization](./novann/utils/visualizations/visualization.py)) para graficar los resultados de una manera m√°s presentable.

![image](./images/metrics.png)

## üìÇ Estructura del proyecto

[NovaNN Structure](./NovaNNFiletree.md)

```
üìÅ NovaNN
‚îú‚îÄ‚îÄ üìÅ data
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ FashionMnist
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ Mnist
‚îÇ       ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ üìÅ examples
‚îÇ   ‚îú‚îÄ‚îÄ üêç binary_classification.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç conv_example.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç multiclass_classification.py
‚îÇ   ‚îî‚îÄ‚îÄ üêç regresion.py
‚îú‚îÄ‚îÄ üìÅ images
‚îÇ   ‚îî‚îÄ‚îÄ üñºÔ∏è metrics.png
‚îú‚îÄ‚îÄ üìÅ notebooks
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ exploration.ipynb
‚îú‚îÄ‚îÄ üìÅ novann
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ _typing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç _typing.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ core
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç config.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç constants.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç init.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ layers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ activations
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç activations.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç relu.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç sigmoid.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç softmax.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç tanh.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ bn
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç batchnorm1d.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç batchnorm2d.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ convolutional
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç conv1d.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç conv2d.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ flatten
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç flatten.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ linear
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç linear.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ pooling
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ gap
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç global_avg_pool1d.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç global_avg_pool2d.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ maxpool
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç maxpool1d.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç maxpool2d.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ regularization
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç dropout.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ losses
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç functional.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ metrics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç nn.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ module
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç layer.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç module.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ optim
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç adam.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç rmsprop.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç sgd.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ utils
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ decorators
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç timing.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ gradient_checking
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç numerical.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ log_config
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç logger.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ train
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç train.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ visualizations
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç visualization.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ üêç __init__.py
‚îú‚îÄ‚îÄ üìÅ tests
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ initializers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_init.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ layers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ activations
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_leaky_relu.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_relu.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_sigmoid.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_softmax.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_tanh.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ batch_norm
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_batchnorm1d.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_batchnorm2d.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ conv
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_conv1d.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_conv2d.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ linear
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_linear.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ pooling
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ gap
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_gap1d.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_gap2d.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ maxpool
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üêç test_maxpooling1d.py
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üêç test_maxpooling2d.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ regularization
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üêç test_dropout.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ optimizers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_adam.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_rmsprop.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_sgd.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ sequential
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_sequential.py
‚îÇ   ‚îú‚îÄ‚îÄ üìù README.en.md
‚îÇ   ‚îî‚îÄ‚îÄ üìù README.md
‚îú‚îÄ‚îÄ ‚öôÔ∏è .gitignore
‚îú‚îÄ‚îÄ üìÑ LICENCE
‚îú‚îÄ‚îÄ üìù NovaNNFiletree.md
‚îú‚îÄ‚îÄ üìù README.en.md
‚îú‚îÄ‚îÄ üìù README.md
‚îú‚îÄ‚îÄ üêç main.py
‚îú‚îÄ‚îÄ üìÑ poetry.lock
‚îî‚îÄ‚îÄ ‚öôÔ∏è pyproject.toml
```

## Estructura del modulo `novann/` y sub directorios

Aqu√≠ que se va a explicar a detalle que hace cada submodulo y sus

### `üìÇ _typing/`

**Definiciones de tipos para el sistema de tipado est√°tico**

Contiene:
- `_typing.py`: Tipos personalizados para tensores, inicializadores, par√°metros, etc.

#### `_typing.py`

- **Prop√≥sito**: Definiciones de tipos (type hints) para todo el framework
- **Tipos principales**:
  - `Shape`: Forma de tensores (tupla de enteros)
  - `InitFn`: Firma de funci√≥n de inicializaci√≥n de pesos
  - `ListOfParameters`: Lista de par√°metros entrenables
  - `IntOrPair`: Entero o tupla para dimensiones flexibles
  - `KernelSize`, `Stride`, `Padding`: Tipos para capas convolucionales
  - `Optimizer`: Alias para optimizadores (Adam, SGD, RMSprop)
  - `LossFunc`: Alias para funciones de p√©rdida
  - `Loader`: Tipo para dataloaders iterables
- **Uso en el framework**: Estos tipos son importados por todos los m√≥dulos para anotaciones de tipo consistentes
- **Conexiones**:
  - `InitFn` es utilizado por `config.py` para los mapas de inicializaci√≥n
  - `ListOfParameters` es usado por las capas que retornan parametros entreneables
  - Los tipos convolucionales son usados por `Conv1d`, `Conv2d`, `MaxPool`, etc.


### `üìÇ core/`

**Configuraci√≥n global, inicializaci√≥n de pesos y constantes del framework**

Contiene:
- `config.py`: Mapas de inicializaci√≥n de pesos basados en activaciones
- `init.py`: Funciones de inicializaci√≥n (Xavier, Kaiming, random)
- `constants.py`: Variables de entorno y rutas de datasets

#### `config.py`

- **Prop√≥sito**: Configuraci√≥n centralizada de inicializaci√≥n de pesos para el framework
- **Diccionarios de inicializaci√≥n**:
  - `DEFAULT_NORMAL_INIT_MAP`: Mapeo de funciones de inicializaci√≥n con distribuci√≥n normal para diferentes funciones de activaci√≥n
  - `DEFAULT_UNIFORM_INIT_MAP`: Mapeo de funciones de inicializaci√≥n con distribuci√≥n uniforme para diferentes funciones de activaci√≥n
- **Claves soportadas**: `relu`, `leakyrelu`, `tanh`, `sigmoid`, `default` (para inicializaci√≥n por defecto)
- **Integraci√≥n con `core/init.py`**: Utiliza las funciones de inicializaci√≥n (`kaiming_normal_`, `kaiming_uniform_`, `xavier_normal_`, `xavier_uniform_`, `random_init_`) y `calculate_gain` para calcular ganancias apropiadas
- **Uso en capas lineales**: Los mapas son utilizados por `Linear.reset_parameters()` para inicializar pesos y sesgos bas√°ndose en la activaci√≥n adyacente
- **Uso en capas convolucionales**: Tambi√©n empleados por `Conv1d` y `Conv2d` para inicializar kernels convolucionales siguiendo el mismo principio
- **Uso en Sequential**: El contenedor `Sequential` utiliza estos mapas para inicializaci√≥n autom√°tica de capas lineales y convolucionales en funci√≥n de las activaciones circundantes
- **Detalles por activaci√≥n**:
  - **ReLU**: Inicializaci√≥n Kaiming normal/uniforme con `a=0.0`
  - **LeakyReLU**: Inicializaci√≥n Kaiming normal/uniforme con `a=0.01` (pendiente negativa)
  - **Tanh**: Inicializaci√≥n Xavier normal/uniforme con ganancia calculada para tanh
  - **Sigmoid**: Inicializaci√≥n Xavier normal/uniforme con ganancia calculada para sigmoid
  - **Default**: Inicializaci√≥n aleatoria peque√±a (conservadora) para casos no especificados


#### `init.py`

- **Prop√≥sito**: Funciones de inicializaci√≥n de pesos (Xavier/Glorot, Kaiming/He y aleatoria)
- **Uso en capas**: Utilizadas por capas lineales y convolucionales bas√°ndose en la activaci√≥n siguiente
- **Funciones**:
  - `calculate_gain(nonlinearity, param)`: Calcula ganancia para activaciones
  - `xavier_normal_(shape, gain)`: Inicializaci√≥n Xavier normal
  - `xavier_uniform_(shape, gain)`: Inicializaci√≥n Xavier uniforme
  - `kaiming_normal_(shape, a, nonlinearity, mode)`: Inicializaci√≥n Kaiming normal
  - `kaiming_uniform_(shape, a, nonlinearity, mode)`: Inicializaci√≥n Kaiming uniforme
  - `random_init_(shape, gain)`: Inicializaci√≥n aleatoria peque√±a (default conservador)
- **Integraci√≥n con `config.py`**: Estas funciones son mapeadas por `DEFAULT_NORMAL_INIT_MAP` y `DEFAULT_UNIFORM_INIT_MAP`

#### `constants.py`

- **Prop√≥sito**: Variables de configuraci√≥n global desde archivo `.env`
- **Contenido**:
  - Rutas de datasets Fashion-MNIST y MNIST
  - Configuraci√≥n de logging (archivo, formato, nivel)
- **Uso en el framework**: Importadas por otros m√≥dulos para acceso a configuraci√≥n


### `üìÇ layers/`

**Implementaciones de todas las capas de red neuronal (lineales, convolucionales, pooling, normalizaci√≥n, activaciones y regularizaci√≥n)**

Contiene subdirectorios organizados por tipo de capa:
- `activations/`: Funciones de activaci√≥n (ReLU, Sigmoid, Tanh, Softmax)
- `linear/`: Capas lineales (fully connected)
- `convolutional/`: Capas convolucionales 1D y 2D
- `pooling/`: Capas de pooling (MaxPool, GlobalAvgPool)
- `bn/`: Normalizaci√≥n por lotes (BatchNorm)
- `regularization/`: Regularizaci√≥n (Dropout)
- `flatten/`: Capa para aplanar tensores

Todas las capas heredan de `Layer` y siguen la interfaz forward/backward est√°ndar.

#### `üìÇ layers/üìÇ activations/`

**Clases base e implementaciones de funciones de activaci√≥n**

Contiene:
- `activations.py`: Clase base `Activation` para todas las activaciones
- `relu.py`: Implementaciones de `ReLU` y `LeakyReLU`
- `sigmoid.py`: Implementaci√≥n de `Sigmoid`
- `softmax.py`: Implementaci√≥n de `Softmax`
- `tanh.py`: Implementaci√≥n de `Tanh`

##### `activations.py`

- **Prop√≥sito**: Clase base para todas las capas de activaci√≥n
- **Clase principal**: `Activation` (hereda de `Layer`)
- **Atributos**:
  - `name`: Nombre en min√∫sculas de la clase (identificador para mapas de inicializaci√≥n)
  - `affect_init`: Booleano que indica si la activaci√≥n influye en la inicializaci√≥n de pesos
- **M√©todos**:
  - `get_init_key()`: Retorna la clave de inicializaci√≥n si `affect_init = True`
  - `init_key`: Propiedad que es alias de `get_init_key()`
- **Conexiones**:
  - Hereda de `Layer` (`novann.module.layer`)
  - Usado por todas las activaciones concretas (ReLU, Sigmoid, etc.)
  - El atributo `affect_init` y `get_init_key()` son utilizados por `Sequential` para inicializaci√≥n autom√°tica basada en activaciones

##### `relu.py`

- **Prop√≥sito**: Implementaciones de ReLU y LeakyReLU
- **Clases**:
  - `ReLU`: Rectified Linear Unit (max(0, x))
  - `LeakyReLU`: Leaky ReLU con pendiente negativa configurable
- **Atributos**:
  - `ReLU._mask`: M√°scara booleana guardada durante el forward (x > 0)
  - `LeakyReLU.a`: Pendiente negativa para valores negativos
  - `LeakyReLU.activation_param`: Almacena el mismo valor que `a` (para consistencia)
  - `LeakyReLU._cache_input`: Entrada guardada para backward
- **Conexiones**:
  - Ambas clases heredan de `Activation`
  - `affect_init = True` en ambas, por lo que influyen en la inicializaci√≥n de pesos
  - Utilizan `kaiming_normal_`/`kaiming_uniform_` de `config.py` para inicializaci√≥n (mapeadas por `relu` y `leakyrelu`)
- **Implementaci√≥n**:
  - `ReLU.forward`: Aplica `max(0, x)` y guarda m√°scara
  - `ReLU.backward`: Propaga gradientes solo donde la entrada fue > 0
  - `LeakyReLU.forward`: Aplica `x si x >= 0, sino a * x`
  - `LeakyReLU.backward`: Gradiente es `1.0` para x ‚â• 0, `a` para x < 0

##### `sigmoid.py`

- **Prop√≥sito**: Implementaci√≥n de la funci√≥n sigmoide
- **Clase**: `Sigmoid` (hereda de `Activation`)
- **Atributos**:
  - `out`: Salida guardada del forward para usar en backward
  - `affect_init = True`: Afecta la inicializaci√≥n de pesos
- **Conexiones**:
  - Usa `xavier_normal_`/`xavier_uniform_` de `config.py` para inicializaci√≥n (mapeada por `sigmoid`)
  - Utiliza ganancia calculada por `calculate_gain` de `init.py`
- **Implementaci√≥n**:
  - `forward`: Calcula `1 / (1 + exp(-x))` y guarda en `out`
  - `backward`: Calcula gradiente usando `out * (1 - out)`

##### `softmax.py`

- **Prop√≥sito**: Implementaci√≥n de softmax num√©ricamente estable
- **Clase**: `Softmax` (hereda de `Activation`)
- **Atributos**:
  - `axis`: Eje sobre el cual aplicar softmax (por defecto 1)
  - `out`: Salida guardada del forward
  - `affect_init = False`: No afecta la inicializaci√≥n de pesos (dise√±ada para usarse con `CrossEntropyLoss`)
- **Conexiones**:
  - Normalmente se usa con `CrossEntropyLoss` que combina softmax y p√©rdida
  - No tiene entrada en los mapas de inicializaci√≥n de `config.py`
- **Implementaci√≥n**:
  - `forward`: Softmax num√©ricamente estable (resta m√°ximo antes de exponenciar)
  - `backward`: Calcula producto Jacobiano-vector eficiente usando salida cacheada

##### `tanh.py`

- **Prop√≥sito**: Implementaci√≥n de tangente hiperb√≥lica
- **Clase**: `Tanh` (hereda de `Activation`)
- **Atributos**:
  - `out`: Salida guardada del forward para usar en backward
  - `affect_init = True`: Afecta la inicializaci√≥n de pesos
- **Conexiones**:
  - Usa `xavier_normal_`/`xavier_uniform_` de `config.py` para inicializaci√≥n (mapeada por `tanh`)
  - Utiliza ganancia calculada por `calculate_gain` de `init.py`
- **Implementaci√≥n**:
  - `forward`: Calcula `tanh(x)` y guarda en `out`
  - `backward`: Calcula gradiente usando `1 - tanh(x)^2`

#### `üìÇ layers/üìÇ bn/`

**Implementaciones de Batch Normalization para diferentes dimensiones de entrada**

Contiene:
- `batchnorm1d.py`: Batch Normalization para entradas 1D/2D (fully connected y convoluciones 1D)
- `batchnorm2d.py`: Batch Normalization para entradas 2D convolucionales (4D tensores)

##### `batchnorm1d.py`

- **Prop√≥sito**: Implementaci√≥n de Batch Normalization para entradas 1D/2D, compatible con capas fully connected y convoluciones 1D
- **Clase principal**: `BatchNorm1d`
- **Caracter√≠sticas principales**:
  - Soporte para entradas 2D `(batch, features)` y 3D `(batch, channels, sequence_length)`
  - Modos distintos para entrenamiento (estad√≠sticas del batch) y evaluaci√≥n (estad√≠sticas m√≥viles)
  - Par√°metros aprendibles `gamma` (escala) y `beta` (desplazamiento) con inicializaci√≥n por defecto (1s y 0s)
  - T√©rmino de estabilidad num√©rica `eps` y correcci√≥n de Bessel para varianza insesgada
  - Momentum configurable para actualizaci√≥n de estad√≠sticas m√≥viles
- **Integraci√≥n**:
  - Hereda de `Layer` de `novann.module`
  - Utiliza `Parameters` de `novann.module` para los par√°metros entrenables `gamma` y `beta`
  - Usa `ListOfParameters` de `novann._typing` para el retorno de par√°metros
  - Los par√°metros `gamma` y `beta` son excluidos autom√°ticamente del weight decay en optimizadores
- **Uso en el framework**:
  - Despu√©s de capas `Linear` en redes fully connected para estabilizar activaciones
  - Despu√©s de capas `Conv1d` en redes convolucionales 1D para normalizaci√≥n por canal
- **Detalles t√©cnicos**:
  - **Algoritmo (Training Mode)**:

    **C√°lculo de estad√≠sticas del batch**:
    
    $$\mu = \frac{1}{m} \sum_{i=1}^{m} x_i$$
    
    $$\sigma^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu)^2$$

    **Correcci√≥n de Bessel (varianza insesgada)**:
    
    $$\sigma_{\text{unbiased}}^2 = \sigma^2 \cdot \frac{m}{m - 1} \quad \text{(si } m > 1\text{)}$$

    **Normalizaci√≥n**:
    
    $$\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma_{\text{unbiased}}^2 + \epsilon}}$$

    **Escala y desplazamiento**:
    
    $$y_i = \gamma \hat{x}_i + \beta$$

    **Actualizaci√≥n de estad√≠sticas m√≥viles**:
    
    $$\text{running\_mean} = (1 - \text{momentum}) \cdot \text{running\_mean} + \text{momentum} \cdot \mu$$
    
    $$\text{running\_var} = (1 - \text{momentum}) \cdot \text{running\_var} + \text{momentum} \cdot \sigma_{\text{unbiased}}^2$$

  - **Algoritmo (Evaluation Mode)**:

    **Normalizaci√≥n con estad√≠sticas m√≥viles**:
    
    $$\hat{x}_i = \frac{x_i - \text{running\_mean}}{\sqrt{\text{running\_var} + \epsilon}}$$
    
    $$y_i = \gamma \hat{x}_i + \beta$$

  - **Backward Pass**:

    **Gradientes de par√°metros**:
    
    $$\frac{\partial L}{\partial \gamma} = \sum_{i=1}^{m} \frac{\partial L}{\partial y_i} \cdot \hat{x}_i$$
    
    $$\frac{\partial L}{\partial \beta} = \sum_{i=1}^{m} \frac{\partial L}{\partial y_i}$$

    **Gradiente respecto a la entrada** (versi√≥n vectorizada eficiente):
    
    $$\frac{\partial L}{\partial x_i} = \frac{\gamma}{m \sqrt{\sigma^2 + \epsilon}} \left( m \cdot \frac{\partial L}{\partial \hat{x}_i} - \sum_{j=1}^{m} \frac{\partial L}{\partial \hat{x}_j} - \hat{x}_i \sum_{j=1}^{m} \frac{\partial L}{\partial \hat{x}_j} \hat{x}_j \right)$$

  - **Manejo de dimensiones**:
    - **2D inputs**: Ejes de reducci√≥n = `(0,)` (batch)
    - **3D inputs**: Ejes de reducci√≥n = `(0, 2)` (batch y sequence)

##### `batchnorm2d.py`

- **Prop√≥sito**: Implementaci√≥n de Batch Normalization para entradas 2D convolucionales (tensores 4D)
- **Clase principal**: `BatchNorm2d`
- **Caracter√≠sticas principales**:
  - Dise√±ado espec√≠ficamente para tensores 4D `(batch, channels, height, width)`
  - Normalizaci√≥n por canal sobre dimensiones espaciales y de batch
  - Par√°metros `gamma` y `beta` con forma `[1, channels, 1, 1]` para broadcasting
  - Modos training/evaluation con comportamiento diferenciado
- **Integraci√≥n**:
  - Hereda de `Layer` de `novann.module`
  - Utiliza `Parameters` para `gamma` y `beta` con shape adaptado a tensores 4D
  - Compatible con capas `Conv2d` para redes convolucionales 2D
- **Uso en el framework**:
  - Despu√©s de capas `Conv2d` en redes convolucionales 2D
  - Normaliza activaciones por canal antes de funciones de activaci√≥n
- **Detalles t√©cnicos**:
  - **Algoritmo (Training Mode)**:

    **C√°lculo de estad√≠sticas por canal**:
    
    $$\mu_c = \frac{1}{m \cdot H \cdot W} \sum_{n=1}^{m} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{nchw}$$
    
    $$\sigma_c^2 = \frac{1}{m \cdot H \cdot W} \sum_{n=1}^{m} \sum_{h=1}^{H} \sum_{w=1}^{W} (x_{nchw} - \mu_c)^2$$

    **Normalizaci√≥n por canal**:
    
    $$\hat{x}_{nchw} = \frac{x_{nchw} - \mu_c}{\sqrt{\sigma_c^2 + \epsilon}}$$
    
    $$y_{nchw} = \gamma_c \hat{x}_{nchw} + \beta_c$$

  - **Algoritmo (Evaluation Mode)**:

    **Normalizaci√≥n con estad√≠sticas m√≥viles por canal**:
    
    $$\hat{x}_{nchw} = \frac{x_{nchw} - \text{running\_mean}_c}{\sqrt{\text{running\_var}_c + \epsilon}}$$
    
    $$y_{nchw} = \gamma_c \hat{x}_{nchw} + \beta_c$$

  - **Backward Pass**:

    **Gradientes de par√°metros por canal**:
    
    $$\frac{\partial L}{\partial \gamma_c} = \sum_{n=1}^{m} \sum_{h=1}^{H} \sum_{w=1}^{W} \frac{\partial L}{\partial y_{nchw}} \cdot \hat{x}_{nchw}$$
    
    $$\frac{\partial L}{\partial \beta_c} = \sum_{n=1}^{m} \sum_{h=1}^{H} \sum_{w=1}^{W} \frac{\partial L}{\partial y_{nchw}}$$

    **Gradiente respecto a la entrada** (similar a `BatchNorm1d` pero reduciendo sobre `(0, 2, 3)`):
    
    $$\frac{\partial L}{\partial x_{nchw}} = \frac{\gamma_c}{m \cdot H \cdot W \cdot \sqrt{\sigma_c^2 + \epsilon}} \left( m \cdot H \cdot W \cdot \frac{\partial L}{\partial \hat{x}_{nchw}} - \sum_{n',h',w'} \frac{\partial L}{\partial \hat{x}_{n'ch'w'}} - \hat{x}_{nchw} \sum_{n',h',w'} \frac{\partial L}{\partial \hat{x}_{n'ch'w'}} \hat{x}_{n'ch'w'} \right)$$

#### `üìÇ layers /üìÇ convolutional/`

**Implementaciones de capas convolucionales para procesamiento de se√±ales 1D y 2D**

Contiene:
- `conv1d.py`: Capa convolucional 1D para procesamiento de secuencias y se√±ales temporales
- `conv2d.py`: Capa convolucional 2D para procesamiento de im√°genes y datos espaciales

##### `conv1d.py`

- **Prop√≥sito**: Implementa una capa convolucional 1D para procesamiento de secuencias y se√±ales temporales
- **Clase principal**: `Conv1d`
- **Caracter√≠sticas principales**:
  - Soporte para entradas 3D `(batch_size, channels, sequence_length)`
  - Kernel convolucional 1D con tama√±o configurable
  - Stride y padding configurable a lo largo de la dimensi√≥n temporal
  - M√∫ltiples modos de padding igual que `Conv2d`
  - Implementaci√≥n eficiente v√≠a `im2col` similar a `Conv2d`
  - Inicializaci√≥n con `DEFAULT_UNIFORM_INIT_MAP["relu"]` por defecto
- **Integraci√≥n**:
  - Hereda de `Layer` de `novann.module`
  - Usa `Parameters` para pesos y biases entrenables
  - Utiliza tipos personalizados de `novann._typing`
  - Compatible con `BatchNorm1d` para normalizaci√≥n por lotes en secuencias
  - Se puede usar con capas de pooling 1D (`MaxPool1d`, `GlobalAvgPool1d`)
- **Uso en el framework**:
  - Para procesamiento de secuencias temporales (audio, series de tiempo)
  - Como componente en redes convolucionales 1D
- **Detalles t√©cnicos**:

  **Transformaci√≥n im2col para 1D**:
  
  $$\text{col} = \text{im2col}(x) \quad \text{(forma: } C_{in} \times K \text{, } N \times L_{out})$$
  
  $$W_{col} = \text{reshape}(W) \quad \text{(forma: } C_{out} \text{, } C_{in} \times K)$$
  
  $$\text{out} = W_{col} \times \text{col} \quad \text{(forma: } C_{out} \text{, } N \times L_{out})$$

  **C√°lculo de longitud de salida**:
  
  $$L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - K}{\text{stride}}\right\rfloor + 1$$

  **Gradientes** (similar a `Conv2d` pero en 1D):
  
  $$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \text{out}} \times \text{col}^T$$
  
  $$\frac{\partial L}{\partial \text{bias}} = \sum_{n,l} \frac{\partial L}{\partial \text{out}}$$
  
  $$\frac{\partial L}{\partial x} = \text{col2im}\left(W_{col}^T \times \frac{\partial L}{\partial \text{out}}\right)$$

##### `conv2d.py`

- **Prop√≥sito**: Implementa una capa convolucional 2D que aplica convoluciones sobre entradas con m√∫ltiples canales (im√°genes)
- **Clase principal**: `Conv2d`
- **Caracter√≠sticas principales**:
  - Soporte para entradas 4D `(batch_size, channels, height, width)`
  - Kernel convolucional 2D con tama√±o configurable `(KH, KW)`
  - Stride y padding configurable en ambas dimensiones
  - M√∫ltiples modos de padding: `zeros`, `reflect`, `replicate`, `circular`
  - Inicializaci√≥n de pesos usando `DEFAULT_UNIFORM_INIT_MAP` de `config.py`
  - Implementaci√≥n eficiente usando transformaci√≥n `im2col` y multiplicaci√≥n de matrices
- **Integraci√≥n**:
  - Hereda de `Layer` de `novann.module`
  - Usa `Parameters` para pesos y biases entrenables
  - Utiliza tipos personalizados de `novann._typing` (`KernelSize`, `Stride`, `Padding`, etc.)
  - Se inicializa con `DEFAULT_UNIFORM_INIT_MAP["relu"]` por defecto (configurable)
  - Compatible con `BatchNorm2d` para normalizaci√≥n por lotes
- **Uso en el framework**:
  - Para procesamiento de im√°genes en redes convolucionales
  - Como componente principal en arquitecturas CNN para visi√≥n por computadora
  - Usado en combinaci√≥n con capas de pooling y normalizaci√≥n
- **Detalles t√©cnicos**:

  **Transformaci√≥n im2col** (convoluci√≥n como multiplicaci√≥n de matrices):
  
  $$\text{col} = \text{im2col}(x) \quad \text{(forma: } C_{in} \times K_H \times K_W \text{, } N \times H_{out} \times W_{out})$$
  
  $$W_{col} = \text{reshape}(W) \quad \text{(forma: } C_{out} \text{, } C_{in} \times K_H \times K_W)$$
  
  $$\text{out} = W_{col} \times \text{col} \quad \text{(forma: } C_{out} \text{, } N \times H_{out} \times W_{out})$$

  **C√°lculo de dimensiones de salida**:
  
  $$H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}_H - K_H}{\text{stride}_H}\right\rfloor + 1$$
  
  $$W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}_W - K_W}{\text{stride}_W}\right\rfloor + 1$$

  **Gradientes en backward pass**:
  
  $$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial \text{out}} \times \text{col}^T$$
  
  $$\frac{\partial L}{\partial \text{bias}} = \sum_{n,h,w} \frac{\partial L}{\partial \text{out}}$$
  
  $$\frac{\partial L}{\partial x} = \text{col2im}\left(W_{col}^T \times \frac{\partial L}{\partial \text{out}}\right)$$

  **Eficiencia**: Ambas implementaciones (`Conv1d` y `Conv2d`) utilizan la transformaci√≥n `im2col` para convertir la operaci√≥n de convoluci√≥n en una multiplicaci√≥n de matrices, lo cual permite un c√°lculo m√°s eficiente al aprovechar bibliotecas optimizadas de √°lgebra lineal.

#### `üìÇ layers/üìÇ flatten/`

**Capa para aplanar tensores, utilizada para transici√≥n entre capas convolucionales/pooling y capas fully connected**

Contiene:
- `flatten.py`: Implementaci√≥n de la capa `Flatten`

##### `flatten.py`

- **Prop√≥sito**: Implementa una capa que aplana tensores manteniendo la dimensi√≥n de batch, utilizada para conectar capas convolucionales/pooling a capas fully connected
- **Clase principal**: `Flatten`
- **Caracter√≠sticas principales**:
  - Aplana todas las dimensiones excepto la dimensi√≥n de batch (axis 0)
  - No tiene par√°metros entrenables (solo transformaci√≥n de forma)
  - Guarda la forma original para el backward pass (des-aplanado)
  - Operaci√≥n puramente de reshape sin c√°lculos costosos
- **Integraci√≥n**:
  - Hereda de `Layer` de `novann.module`
  - No utiliza `Parameters` ya que no tiene par√°metros entrenables
  - Dise√±ada para usarse entre capas convolucionales (`Conv2d`, `MaxPool2d`) y capas lineales (`Linear`)
- **Uso en el framework**:
  - En arquitecturas CNN para conectar la salida de capas convolucionales/pooling a capas fully connected
  - Necesaria cuando se pasa de tensores multidimensionales (im√°genes) a vectores para clasificaci√≥n
  - Ejemplo t√≠pico en CNNs: `Conv2d -> ReLU -> MaxPool2d -> Flatten -> Linear`
- **Detalles t√©cnicos**:

  **Operaci√≥n forward**:
  
  $$\text{flatten}(x) = \text{reshape}(x, (N, -1))$$
  
  donde $N$ es el tama√±o de batch y $-1$ indica el producto de todas las dimensiones restantes.

  **Operaci√≥n backward**:
  
  $$\frac{\partial L}{\partial x} = \text{reshape}\left(\frac{\partial L}{\partial \text{out}}, \text{shape\_original}\right)$$

  La capa simplemente guarda la forma original durante el forward y la restaura durante el backward, manteniendo el flujo de gradientes.

#### `üìÇ layers/üìÇ linear/`

**Implementaci√≥n de capas totalmente conectadas (fully connected) para transformaciones lineales**

Contiene:
- `linear.py`: Implementaci√≥n de la capa `Linear` para transformaciones lineales

##### `linear.py`

- **Prop√≥sito**: Implementa una capa lineal (fully connected) que realiza la transformaci√≥n $y = xW^T + b$
- **Clase principal**: `Linear`
- **Caracter√≠sticas principales**:
  - Transformaci√≥n lineal completa entre espacios de caracter√≠sticas
  - Soporte opcional para t√©rmino de bias (sesgo)
  - Inicializaci√≥n de pesos usando `DEFAULT_NORMAL_INIT_MAP` de `config.py`
  - Cache de entrada para c√°lculo eficiente de gradientes en backward pass
  - Implementaci√≥n vectorizada usando multiplicaci√≥n de matrices
- **Integraci√≥n**:
  - Hereda de `Layer` de `novann.module`
  - Usa `Parameters` para pesos (`weight`) y bias (`bias`) entrenables
  - Utiliza tipos `ListOfParameters` e `InitFn` de `novann._typing`
  - Se inicializa con `DEFAULT_NORMAL_INIT_MAP["default"]` por defecto (configurable)
  - Usado extensivamente por `Sequential` en arquitecturas de redes neuronales
- **Uso en el framework**:
  - Como capa final en redes de clasificaci√≥n/regresi√≥n
  - En redes fully connected (MLP) para transformaciones entre capas ocultas
  - Despu√©s de capas `Flatten` en arquitecturas CNN
  - En combinaci√≥n con funciones de activaci√≥n (`ReLU`, `Sigmoid`, `Tanh`)
- **Detalles t√©cnicos**:

  **Forward pass**:
  
  $$y = x \cdot W^T + b$$
  
  donde:
  - $x \in \mathbb{R}^{N \times D_{in}}$ (entrada)
  - $W \in \mathbb{R}^{D_{out} \times D_{in}}$ (pesos)
  - $b \in \mathbb{R}^{1 \times D_{out}}$ (bias, opcional)
  - $y \in \mathbb{R}^{N \times D_{out}}$ (salida)

  **Backward pass** (c√°lculo de gradientes):

  Gradiente respecto a pesos:
  
  $$\frac{\partial L}{\partial W} = \left(\frac{\partial L}{\partial y}\right)^T \cdot x$$

  Gradiente respecto a bias (si existe):
  
  $$\frac{\partial L}{\partial b} = \sum_{i=1}^{N} \frac{\partial L}{\partial y_i}$$

  Gradiente respecto a entrada:
  
  $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot W$$

  **Inicializaci√≥n**: Por defecto usa `DEFAULT_NORMAL_INIT_MAP["default"]` que corresponde a `random_init_` de `core/init.py`, pero puede ser sobrescrita por inicializadores espec√≠ficos basados en activaciones adyacentes cuando se usa en `Sequential`.

  **Eficiencia**: Utiliza multiplicaci√≥n de matrices optimizada (`@` operator) y mantiene la entrada cacheada para evitar recomputaci√≥n durante el backward pass.

#### `üìÇ layers/üìÇ pooling/`

**Implementaciones de capas de pooling (reducci√≥n dimensional) para extracci√≥n de caracter√≠sticas**

Contiene dos subdirectorios:
- `gap/`: Global Average Pooling (1D y 2D)
- `maxpool/`: Max Pooling (1D y 2D)

##### `üìÇ layers/üìÇ pooling/üìÇ gap/`

**Implementaciones de Global Average Pooling para reducci√≥n a caracter√≠sticas globales**

Contiene:
- `global_avg_pool1d.py`: Global Average Pooling 1D para secuencias
- `global_avg_pool2d.py`: Global Average Pooling 2D para im√°genes

###### `global_avg_pool1d.py`

- **Prop√≥sito**: Implementa Global Average Pooling 1D que promedia a lo largo de la dimensi√≥n temporal para cada canal
- **Clase principal**: `GlobalAvgPool1d`
- **Caracter√≠sticas principales**:
  - Reduce tensores 3D `(batch, channels, length)` a 3D `(batch, channels, 1)`
  - No tiene par√°metros entrenables (operaci√≥n de reducci√≥n fija)
  - Guarda la forma original para backward pass
  - Distribuye gradientes uniformemente en backward
- **Integraci√≥n**:
  - Hereda de `Layer` de `novann.module`
  - Sin par√°metros entrenables, por lo que no aparece en `parameters()`
  - Se usa t√≠picamente al final de redes convolucionales 1D
- **Uso en el framework**:
  - En arquitecturas de redes convolucionales 1D para reducir secuencias a caracter√≠sticas globales
  - Como capa final antes de capas fully connected en tareas de clasificaci√≥n de secuencias
- **Detalles t√©cnicos**:

  **Forward pass**:
  
  $$\text{output}_{n,c} = \frac{1}{L} \sum_{l=1}^{L} x_{n,c,l}$$

  **Backward pass**:
  
  $$\frac{\partial L}{\partial x_{n,c,l}} = \frac{1}{L} \cdot \frac{\partial L}{\partial \text{output}_{n,c}}$$

  donde $n$ es el √≠ndice de batch, $c$ el canal, $l$ la posici√≥n en la secuencia, y $L$ la longitud original.

###### `global_avg_pool2d.py`

- **Prop√≥sito**: Implementa Global Average Pooling 2D que promedia a lo largo de las dimensiones espaciales para cada canal
- **Clase principal**: `GlobalAvgPool2d`
- **Caracter√≠sticas principales**:
  - Reduce tensores 4D `(batch, channels, height, width)` a 4D `(batch, channels, 1, 1)`
  - No tiene par√°metros entrenables
  - Guarda la forma original para backward pass
  - Distribuye gradientes uniformemente en ambas dimensiones espaciales
- **Integraci√≥n**:
  - Hereda de `Layer` de `novann.module`
  - Sin par√°metros entrenables
  - Se utiliza com√∫nmente en arquitecturas modernas de CNN
- **Uso en el framework**:
  - Al final de redes convolucionales 2D para producir un vector de caracter√≠sticas por canal
  - Para reducir la dimensionalidad antes de la clasificaci√≥n en tareas de visi√≥n por computadora
- **Detalles t√©cnicos**:

  **Forward pass**:
  
  $$\text{output}_{n,c} = \frac{1}{H \times W} \sum_{h=1}^{H} \sum_{w=1}^{W} x_{n,c,h,w}$$

  **Backward pass**:
  
  $$\frac{\partial L}{\partial x_{n,c,h,w}} = \frac{1}{H \times W} \cdot \frac{\partial L}{\partial \text{output}_{n,c}}$$

  donde $H$ y $W$ son las dimensiones espaciales originales.

##### `üìÇ layers/üìÇ pooling/üìÇ maxpool/`

**Implementaciones de Max Pooling para reducci√≥n espacial conservando caracter√≠sticas dominantes**

Contiene:
- `maxpool1d.py`: Max Pooling 1D para secuencias
- `maxpool2d.py`: Max Pooling 2D para im√°genes

###### `maxpool1d.py`

- **Prop√≥sito**: Implementa Max Pooling 1D que reduce la dimensi√≥n temporal tomando el valor m√°ximo en ventanas deslizantes
- **Clase principal**: `MaxPool1d`
- **Caracter√≠sticas principales**:
  - Reduce tensores 3D `(batch, channels, length)` a 3D `(batch, channels, length_out)`
  - Ventana deslizante con tama√±o de kernel, stride y padding configurable
  - Soporte para m√∫ltiples modos de padding: `zeros`, `reflect`, `replicate`, `circular`
  - Implementaci√≥n eficiente usando `as_strided` de NumPy
  - En backward: solo las posiciones con valor m√°ximo reciben gradiente
- **Integraci√≥n**:
  - Hereda de `Layer` de `novann.module`
  - No tiene par√°metros entrenables
  - Usa tipos de `novann._typing` para consistencia
- **Uso en el framework**:
  - En redes convolucionales 1D para reducci√≥n de dimensionalidad y extracci√≥n de caracter√≠sticas robustas
  - Despu√©s de capas de convoluci√≥n 1D para reducir la longitud de secuencia
- **Detalles t√©cnicos**:

  **Forward pass** (para cada ventana):
  
  $$\text{output}_{n,c,l} = \max_{k=1}^{K} x_{n,c, s \cdot l + k - p}$$
  
  donde $s$ es el stride, $K$ el tama√±o del kernel y $p$ el padding.

  **Backward pass**:
  
  $$\frac{\partial L}{\partial x_{n,c,pos}} = \sum_{\substack{l \\ \text{pos es argmax en ventana } l}} \frac{1}{|\text{argmax}|} \cdot \frac{\partial L}{\partial \text{output}_{n,c,l}}$$

  El gradiente se propaga solo a las posiciones que fueron el m√°ximo en su ventana, dividi√©ndose si hay m√∫ltiples m√°ximos.

  **C√°lculo de longitud de salida**:
  
  $$L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - K}{\text{stride}}\right\rfloor + 1$$

###### `maxpool2d.py`

- **Prop√≥sito**: Implementa Max Pooling 2D que reduce las dimensiones espaciales tomando el valor m√°ximo en ventanas deslizantes 2D
- **Clase principal**: `MaxPool2d`
- **Caracter√≠sticas principales**:
  - Reduce tensores 4D `(batch, channels, height, width)` a 4D `(batch, channels, height_out, width_out)`
  - Ventana 2D con tama√±o de kernel, stride y padding configurable en ambas dimensiones
  - Soporte para m√∫ltiples modos de padding
  - Implementaci√≥n con `as_strided` para crear ventanas 2D eficientemente
  - Backward similar a MaxPool1d pero en 2D
- **Integraci√≥n**:
  - Hereda de `Layer` de `novann.module`
  - Usa tipos `IntOrPair` de `novann._typing` para par√°metros
  - Sin par√°metros entrenables
- **Uso en el framework**:
  - En redes convolucionales 2D para reducci√≥n espacial y extracci√≥n de caracter√≠sticas invariantes
  - T√≠picamente despu√©s de capas de convoluci√≥n y activaci√≥n en arquitecturas CNN
- **Detalles t√©cnicos**:

  **Forward pass** (para cada ventana 2D):
  
  $$\text{output}_{n,c,i,j} = \max_{h=1}^{K_H} \max_{w=1}^{K_W} x_{n,c, s_h \cdot i + h - p_h, s_w \cdot j + w - p_w}$$

  **Backward pass**:
  
  Similar a 1D pero en 2D, propagando gradientes solo a posiciones de m√°ximo.

  **C√°lculo de dimensiones de salida**:
  
  $$H_{out} = \left\lfloor\frac{H_{in} + 2 \times p_h - K_H}{s_h}\right\rfloor + 1$$
  
  $$W_{out} = \left\lfloor\frac{W_{in} + 2 \times p_w - K_W}{s_w}\right\rfloor + 1$$

  **Eficiencia**: Ambas implementaciones usan `as_strided` para crear ventanas sin copiar datos. El backward requiere bucles para acumular gradientes, lo que puede ser optimizado en futuras versiones.

#### `üìÇ layers/üìÇ regularization/`

**Implementaciones de t√©cnicas de regularizaci√≥n para prevenir sobreajuste en redes neuronales**

Contiene:
- `dropout.py`: Implementaci√≥n de la capa `Dropout` para regularizaci√≥n por apagado aleatorio de neuronas

##### `dropout.py`

- **Prop√≥sito**: Implementa la t√©cnica de regularizaci√≥n Dropout que aleatoriamente apaga neuronas durante el entrenamiento para prevenir sobreajuste
- **Clase principal**: `Dropout`
- **Caracter√≠sticas principales**:
  - Durante entrenamiento: apaga aleatoriamente elementos de entrada con probabilidad `p` y escala los restantes para preservar activaciones esperadas
  - Durante evaluaci√≥n: act√∫a como identidad (sin dropout)
  - Mantiene consistencia de activaciones esperadas mediante escalado `1/(1-p)`
  - Limpia la m√°scara interna despu√©s del backward para evitar referencias entre batches
  - Soporta probabilidad de dropout en rango `[0.0, 1.0)`
- **Integraci√≥n**:
  - Hereda de `Layer` de `novann.module`
  - No tiene par√°metros entrenables
  - Sobrescribe m√©todos `train()` y `eval()` para gestionar correctamente el modo
  - Compatible con todas las capas del framework que siguen la interfaz est√°ndar
- **Uso en el framework**:
  - Insertado entre capas en redes profundas para regularizaci√≥n
  - T√≠picamente usado despu√©s de capas `Linear` o `Conv` y antes de activaciones
  - √ötil para prevenir sobreajuste en redes con muchas capacidades
- **Detalles t√©cnicos**:

  **Forward pass (Training Mode)**:

  Para cada elemento $x_i$ de la entrada:
  
  1. Generar m√°scara binaria:
     $$m_i \sim \text{Bernoulli}(1-p)$$

  2. Aplicar dropout y escalado:
     $$y_i = \frac{x_i \cdot m_i}{1-p}$$

  **Forward pass (Evaluation Mode)**:
  
  $$y_i = x_i \quad \text{(sin cambios)}$$

  **Backward pass (Training Mode)**:
  
  $$\frac{\partial L}{\partial x_i} = \frac{m_i}{1-p} \cdot \frac{\partial L}{\partial y_i}$$

  **Propiedades estad√≠sticas**:
  
  Durante entrenamiento:
  $$E[y_i] = E\left[\frac{x_i \cdot m_i}{1-p}\right] = x_i \cdot \frac{E[m_i]}{1-p} = x_i \cdot \frac{1-p}{1-p} = x_i$$

  Esto garantiza que la activaci√≥n esperada se mantenga igual durante entrenamiento y evaluaci√≥n.

  **Implementaci√≥n pr√°ctica**:
  - La m√°scara se genera usando `np.random.rand()` y se convierte a booleano
  - Se almacena como el mismo dtype que la entrada para eficiencia
  - Se limpia despu√©s del backward para liberar memoria y evitar referencias cruzadas
  - Las dimensiones se mantienen en todas las operaciones

  **Consideraciones de rendimiento**:
  - El escalado `1/(1-p)` se aplica durante forward y backward para consistencia
  - En modo evaluaci√≥n, no hay overhead computacional
  - La generaci√≥n aleatoria introduce cierta sobrecarga pero es esencial para el efecto de regularizaci√≥n

### `üìÇ losses/`

**Implementaciones de funciones de p√©rdida para diferentes tareas de aprendizaje autom√°tico**

Contiene:
- `functional.py`: Implementaciones de `CrossEntropyLoss`, `MSE`, `MAE`, y `BinaryCrossEntropy`

#### `functional.py`

- **Prop√≥sito**: Contiene implementaciones de funciones de p√©rdida utilizadas para entrenar modelos de deep learning
- **Clases principales**:
  - `CrossEntropyLoss`: P√©rdida de entrop√≠a cruzada para clasificaci√≥n multiclase
  - `MSE`: Error cuadr√°tico medio para regresi√≥n
  - `MAE`: Error absoluto medio para regresi√≥n
  - `BinaryCrossEntropy`: Entrop√≠a cruzada binaria para clasificaci√≥n binaria
- **Caracter√≠sticas comunes**:
  - Todas implementan interfaz `forward()` y `backward()`
  - M√©todo `__call__()` para conveniencia (calcula p√©rdida y gradiente juntos)
  - Cache de valores intermedios para c√°lculo eficiente de gradientes
  - Estabilidad num√©rica con t√©rminos epsilon para evitar log(0)
  - Soporte para batches de datos
- **Integraci√≥n**:
  - Usadas por el sistema de entrenamiento en `novann/utils/train/train.py`
  - Compatibles con todos los modelos que implementan la interfaz est√°ndar
  - `CrossEntropyLoss` utiliza `Softmax` de `layers/activations/` internamente
  - Tipos de entrada compatibles con salidas de capas del framework

###### `CrossEntropyLoss`

- **Prop√≥sito**: P√©rdida de entrop√≠a cruzada para clasificaci√≥n multiclase con etiquetas enteras
- **Caracter√≠sticas**:
  - Espera `logits` de forma `(N, C)` y etiquetas como √≠ndices enteros `(N,)`
  - Usa softmax num√©ricamente estable internamente
  - Maneja tanto etiquetas one-hot como √≠ndices de clase
  - T√©rmino epsilon `eps=1e-12` para evitar log(0)
- **Implementaci√≥n matem√°tica**:

  **Forward pass**:
  
  $$\hat{y} = \text{softmax}(z)$$
  
  $$L = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} y_{ij} \cdot \log(\hat{y}_{ij} + \epsilon)$$

  **Backward pass** (gradiente respecto a logits):
  
  $$\frac{\partial L}{\partial z} = \frac{\hat{y} - y}{N}$$

###### `MSE`

- **Prop√≥sito**: Error cuadr√°tico medio para problemas de regresi√≥n
- **Caracter√≠sticas**:
  - Espera logits y targets con formas id√©nticas
  - Retorna MSE promediado sobre el batch
  - Simple y eficiente computacionalmente
- **Implementaci√≥n matem√°tica**:

  **Forward pass**:
  
  $$L = \frac{1}{N} \sum_{i=1}^{N} (z_i - y_i)^2$$

  **Backward pass**:
  
  $$\frac{\partial L}{\partial z} = \frac{2}{N} (z - y)$$

###### `MAE`

- **Prop√≥sito**: Error absoluto medio para regresi√≥n robusta (menos sensible a outliers que MSE)
- **Caracter√≠sticas**:
  - Espera logits y targets con formas id√©nticas
  - Retorna MAE promediado sobre el batch
  - Uso de `np.sign()` en backward para manejar el valor absoluto
- **Implementaci√≥n matem√°tica**:

  **Forward pass**:
  
  $$L = \frac{1}{N} \sum_{i=1}^{N} |z_i - y_i|$$

  **Backward pass**:
  
  $$\frac{\partial L}{\partial z} = \frac{\text{sign}(z - y)}{N}$$

###### `BinaryCrossEntropy`

- **Prop√≥sito**: Entrop√≠a cruzada binaria para clasificaci√≥n binaria con salidas sigmoid
- **Caracter√≠sticas**:
  - Espera probabilidades `(N, ...)` y targets de misma forma con valores 0/1
  - Dise√±ada para usarse con salidas de `Sigmoid`
  - T√©rmino epsilon para estabilidad num√©rica en logaritmos
- **Implementaci√≥n matem√°tica**:

  **Forward pass**:
  
  $$L = -\frac{1}{N} \sum_{i=1}^{N} \left[y_i \cdot \log(p_i + \epsilon) + (1 - y_i) \cdot \log(1 - p_i + \epsilon)\right]$$

  **Backward pass**:
  
  $$\frac{\partial L}{\partial p} = \frac{p - y}{N}$$

### `üìÇ metrics/`

**Implementaciones de m√©tricas de evaluaci√≥n para diferentes tareas**

Contiene:
- `metrics.py`: Funciones para calcular `accuracy`, `binary_accuracy`, y `r2_score`

#### `metrics.py`

- **Prop√≥sito**: Contiene funciones para evaluar el desempe√±o de modelos en diferentes tareas
- **Funciones principales**:
  - `accuracy()`: Precisi√≥n para clasificaci√≥n multiclase
  - `binary_accuracy()`: Precisi√≥n para clasificaci√≥n binaria
  - `r2_score()`: Coeficiente de determinaci√≥n para regresi√≥n
- **Caracter√≠sticas comunes**:
  - Todas aceptan un modelo y un dataloader como entrada
  - Calculan m√©tricas sobre todo el dataset proporcionado por el dataloader
  - Compatibles con modelos `Sequential` y cualquier callable que siga la interfaz
  - Implementaciones eficientes usando operaciones vectorizadas de NumPy

###### `accuracy`

- **Prop√≥sito**: Calcula la precisi√≥n de clasificaci√≥n para problemas multiclase
- **Implementaci√≥n**:
  - Toma predicciones de modelo y etiquetas verdaderas
  - Usa `np.argmax()` para obtener clases predichas
  - Calcula fracci√≥n de predicciones correctas
  - **F√≥rmula**: $\text{accuracy} = \frac{\text{predicciones correctas}}{\text{total muestras}}$

###### `binary_accuracy`

- **Prop√≥sito**: Calcula la precisi√≥n para problemas de clasificaci√≥n binaria
- **Implementaci√≥n**:
  - Espera probabilidades en rango [0, 1]
  - Usa umbral 0.5 para convertir probabilidades a predicciones binarias
  - Calcula fracci√≥n de predicciones correctas
  - **F√≥rmula**: $\text{binary accuracy} = \frac{\text{predicciones correctas}}{\text{total muestras}}$

###### `r2_score`

- **Prop√≥sito**: Calcula el coeficiente de determinaci√≥n $R^2$ para evaluar modelos de regresi√≥n
- **Implementaci√≥n**:
  - Calcula suma de cuadrados de residuos (SSE) y suma total de cuadrados (SST)
  - Maneja caso donde SST = 0 (varianza nula en datos)
  - **F√≥rmula**:
    
    $$\text{SSE} = \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$
    
    $$\text{SST} = \sum_{i=1}^{N} (y_i - \bar{y})^2$$
    
    $$R^2 = 1 - \frac{\text{SSE}}{\text{SST}}$$
  
  - **Interpretaci√≥n**:
    - $R^2 = 1$: Ajuste perfecto
    - $R^2 = 0$: Modelo igual que predecir la media
    - $R^2 < 0$: Modelo peor que predecir la media

### `üìÇ model/`

**Contenedores y arquitecturas de modelos para construir redes neuronales**

Contiene:
- `nn.py`: Implementaci√≥n del contenedor `Sequential` para construir modelos secuenciales

#### `nn.py`

- **Prop√≥sito**: Implementa el contenedor `Sequential` que permite construir redes neuronales como secuencia de capas, similar al `nn.Sequential` de PyTorch
- **Clase principal**: `Sequential`
- **Caracter√≠sticas principales**:
  - Contenedor secuencial que encadena m√∫ltiples capas en orden
  - Inicializaci√≥n autom√°tica de pesos para capas lineales y convolucionales basada en activaciones adyacentes
  - Gesti√≥n unificada de modos (train/eval) para todas las capas contenidas
  - Recopilaci√≥n autom√°tica de todos los par√°metros entrenables
  - Forward/backward propagaci√≥n secuencial a trav√©s de las capas
  - Sistema de logging integrado para seguimiento de inicializaci√≥n
- **Integraci√≥n**:
  - Hereda de `Layer` de `novann.module` (es una capa que contiene otras capas)
  - Interact√∫a con `DEFAULT_NORMAL_INIT_MAP` de `core/config.py` para obtener funciones de inicializaci√≥n
  - Detecta autom√°ticamente activaciones (`Activation` de `layers/activations/`)
  - Inicializa capas `Linear`, `Conv1d`, y `Conv2d` bas√°ndose en activaciones cercanas
  - Utiliza tipos `InitFn`, `ActivAndParams`, y `Shape` de `novann._typing`
  - Usa `logger` de `novann/utils/log_config/` para logging
- **Uso en el framework**:
  - Contenedor principal para construir arquitecturas de redes neuronales
  - Simplifica la creaci√≥n de modelos secuenciales sin necesidad de implementar forward/backward manualmente
  - Automatiza la inicializaci√≥n √≥ptima de pesos bas√°ndose en las activaciones utilizadas
  - Ejemplo de construcci√≥n de modelos:
    ```python
    model = Sequential(
        Conv2d(3, 64, 3, padding=1, bias=False), # -> (32, 32, 3)
        BatchNorm2d(64),
        ReLU(),
        MaxPool2d(2, 2) # ->  (16, 16, 64)
        Linear(16 * 16 * 64, 10)
    )
    ```
- **Detalles t√©cnicos**:

  **Inicializaci√≥n autom√°tica de pesos**:
  
  El algoritmo busca activaciones adyacentes para determinar la inicializaci√≥n √≥ptima:
  
  1. Para cada capa inicializable (`Linear`, `Conv1d`, `Conv2d`):
     - Busca la siguiente activaci√≥n en la secuencia
     - Si no hay siguiente activaci√≥n, busca la √∫ltima activaci√≥n anterior
     - Usa la clave de inicializaci√≥n (`init_key`) de la activaci√≥n para seleccionar del `DEFAULT_NORMAL_INIT_MAP`
     - Para `LeakyReLU`, inyecta el par√°metro `a` (pendiente negativa) en la inicializaci√≥n
  
  2. Inicializaciones mapeadas:
     - `relu` ‚Üí `kaiming_normal_` con `a=0.0`
     - `leakyrelu` ‚Üí `kaiming_normal_` con `a` espec√≠fico
     - `tanh` ‚Üí `xavier_normal_` con ganancia para tanh
     - `sigmoid` ‚Üí `xavier_normal_` con ganancia para sigmoid
     - `default` ‚Üí `random_init_` (inicializaci√≥n conservadora)

  **Propagaci√≥n forward**:
  
  $$\text{out} = \text{layer}_n(\dots \text{layer}_2(\text{layer}_1(x)))$$
  
  Ejecuta las capas en el orden en que fueron a√±adidas al constructor.

  **Propagaci√≥n backward**:
  
  $$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial \text{layer}_n} \cdot \dots \cdot \frac{\partial \text{layer}_2}{\partial \text{layer}_1} \cdot \frac{\partial \text{layer}_1}{\partial x}$$
  
  Ejecuta backward en orden inverso, propagando gradientes a trav√©s de todas las capas.

  **Gesti√≥n de par√°metros**:
  
  - `parameters()`: Retorna todos los par√°metros de todas las capas entrenables
  - `zero_grad()`: Limpia gradientes de todos los par√°metros
  - Modos `train()` y `eval()`: Configuran todos los subm√≥dulos simult√°neamente

  **Logging**: Usa el sistema de logging del framework para registrar qu√© inicializadores se aplican a cada capa, facilitando debugging y ajuste de hiperpar√°metros.

### `üìÇ module/`

**Clases base fundamentales para el sistema de m√≥dulos y par√°metros de NovaNN**

Contiene:
- `module.py`: Clases base `Module` y `Parameters` para gesti√≥n de par√°metros y modo entrenamiento/evaluaci√≥n
- `layer.py`: Clase abstracta `Layer` que define la interfaz para todas las capas de red neuronal

#### `module.py`

- **Prop√≥sito**: Define las clases base `Module` y `Parameters` que forman la base del sistema de m√≥dulos y gesti√≥n de par√°metros entrenables
- **Clases principales**:
  - `Module`: Clase base para todos los componentes de red neuronal, gestiona modo entrenamiento/evaluaci√≥n y recolecci√≥n de par√°metros
  - `Parameters`: Contenedor para tensores entrenables que almacena datos y gradientes
- **Integraci√≥n**:
  - `Module` es la clase ra√≠z de la jerarqu√≠a de herencia del framework
  - `Parameters` es utilizado por todas las capas con par√°metros entrenables (`Linear`, `Conv1d`, `Conv2d`, `BatchNorm1d`, `BatchNorm2d`)
  - Define el tipo `ListOfParameters` en `novann._typing` para anotaciones de tipo
- **Uso en el framework**:
  - Todas las capas y modelos heredan de `Module`
  - Los par√°metros entrenables se encapsulan en instancias de `Parameters`
  - El sistema de optimizaci√≥n accede a par√°metros a trav√©s del m√©todo `parameters()`

##### `Parameters`

- **Prop√≥sito**: Contenedor para tensores entrenables que almacena tanto los valores del par√°metro como sus gradientes
- **Caracter√≠sticas principales**:
  - Almacena `data`: valores actuales del par√°metro (numpy array)
  - Almacena `grad`: gradientes acumulados (misma forma que `data`)
  - Soporta `name`: opcional para que los optimizadores puedan ignorar los parametros de `BatchNorm` para no aplicarles  weight decay
  - M√©todo `zero_grad()` para reiniciar gradientes
- **Implementaci√≥n**:
  - Inicializa `grad` como array de ceros con misma forma que `data`
  - `zero_grad(set_to_none=True)`: Puede establecer gradientes a `None` o a array de ceros
  - Compatible con optimizadores que esperan acceso a `data` y `grad`

##### `Module`

- **Prop√≥sito**: Clase base para todos los m√≥dulos del framework, proporciona funcionalidad com√∫n para manejo de estado y par√°metros
- **Caracter√≠sticas principales**:
  - Gestiona el estado de entrenamiento/evaluaci√≥n (`_training` flag)
  - Proporciona m√©todos `train()` y `eval()` para cambiar entre modos
  - Define la interfaz `parameters()` para recolectar par√°metros entrenables
  - Implementa `zero_grad()` para limpiar gradientes de todos los par√°metros
- **Implementaci√≥n**:
  - `_training`: Atributo booleano que controla el comportamiento de capas sensibles al modo (Dropout, BatchNorm)
  - `parameters()`: Retorna lista vac√≠a por defecto, debe ser sobrescrito por subclases con par√°metros
  - `zero_grad()`: Itera sobre todos los par√°metros y limpia sus gradientes  (m√°s que todo para su uso en los tests)

#### `layer.py`

- **Prop√≥sito**: Define la clase abstracta `Layer` que establece la interfaz para todas las capas de transformaci√≥n en el framework
- **Clase principal**: `Layer`
- **Caracter√≠sticas principales**:
  - Clase abstracta que hereda de `Module` y `ABC` (Abstract Base Class)
  - Define la interfaz obligatoria `forward()` y `backward()`
  - Implementa `__call__` para permitir sintaxis de llamada directa
  - Base de todas las capas concretas (lineales, convolucionales, activaciones, etc.)
- **Integraci√≥n**:
  - Hereda de `Module` (obtiene funcionalidad de gesti√≥n de estado y par√°metros)
  - Todas las capas implementadas (`Linear`, `Conv2d`, `ReLU`, `BatchNorm1d`, etc.) heredan de `Layer`
  - El contenedor `Sequential` opera con instancias de `Layer`
- **Uso en el framework**:
  - Cada capa concreta implementa `forward()` para la propagaci√≥n hacia adelante
  - Cada capa concreta implementa `backward()` para la propagaci√≥n hacia atr√°s
  - Permite composici√≥n modular de redes a trav√©s de `Sequential`

##### `Layer`

- **Prop√≥sito**: Clase base abstracta que define la interfaz m√≠nima que todas las capas deben implementar
- **M√©todos abstractos**:
  - `forward(x)`: Transformaci√≥n de entrada a salida durante la propagaci√≥n hacia adelante
  - `backward(grad)`: C√°lculo de gradientes respecto a la entrada y par√°metros durante retropropagaci√≥n
- **Implementaci√≥n concreta**:
  - `__call__(x)`: Permite usar instancias de capa como funciones: `output = layer(input)`
  - Delega a `forward()` cuando se llama a la instancia
- **Flujo de trabajo t√≠pico**:
  1. Durante entrenamiento: `output = layer(input)`
  2. Durante retropropagaci√≥n: `grad_input = layer.backward(grad_output)`
  3. Los gradientes de par√°metros se acumulan en los objetos `Parameters` correspondientes

**Jerarqu√≠a completa del sistema de m√≥dulos**:
```
Module (base)
    ‚Ü≥ Layer (abstracta)
        ‚Ü≥ Linear, Activation, BatchNormalization, Dropout
            ‚Ü≥ Activation (base para activaciones)
                ‚Ü≥ ReLU, LeakyReLU, Sigmoid, SoftMax, Tanh
```

Esta jerarqu√≠a permite un dise√±o modular donde cada componente sigue una interfaz consistente, facilitando la composici√≥n y el entrenamiento de redes neuronales complejas.

### `üìÇ optim/`

**Implementaciones de optimizadores para el entrenamiento de redes neuronales**

Contiene:
- `adam.py`: Optimizador Adam (Adaptive Moment Estimation)
- `rmsprop.py`: Optimizador RMSprop (Root Mean Square Propagation)
- `sgd.py`: Optimizador SGD (Stochastic Gradient Descent) con momentum y clipping de gradientes

#### `adam.py`

- **Prop√≥sito**: Implementa el optimizador Adam (Adaptive Moment Estimation) que combina las ventajas de AdaGrad y RMSProp con momentos de primer y segundo orden
- **Clase principal**: `Adam`
- **Caracter√≠sticas principales**:
  - Estimaciones adaptativas de momentos de primer y segundo orden
  - Correcci√≥n de bias para momentos en las primeras iteraciones
  - Soporte para weight decay L1 y L2 con exclusi√≥n autom√°tica de par√°metros de BatchNorm
  - Coeficientes configurables `betas` para las tasas de decaimiento de momentos
  - T√©rmino epsilon para estabilidad num√©rica en la divisi√≥n
- **Integraci√≥n**:
  - Opera sobre listas de `Parameters` de `novann.module`
  - Usa el tipo `ListOfParameters` de `novann._typing`
  - Excluye autom√°ticamente par√°metros `gamma` y `beta` de BatchNorm del weight decay
  - Compatible con todos los modelos que implementan el m√©todo `parameters()`
- **Uso en el framework**:
  - Optimizador por defecto para muchos problemas de deep learning modernos
  - Adecuado para redes con arquitecturas complejas y gran cantidad de par√°metros
  - Utilizado en ejemplos de clasificaci√≥n y regresi√≥n del framework
- **Detalles t√©cnicos**:

  **Algoritmo de actualizaci√≥n**:

  Para cada par√°metro $\theta$ en el paso $t$:

  $$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$
  
  $$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$
  
  $$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$
  
  $$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$
  
  $$\theta_{t+1} = \theta_t - \frac{\eta \cdot \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

  donde:
  - $\eta$: Tasa de aprendizaje (`lr`)
  - $\beta_1, \beta_2$: Coeficientes de decaimiento (`betas`)
  - $g_t$: Gradiente en el paso $t$
  - $\epsilon$: T√©rmino de estabilidad num√©rica (`eps`)

  **Weight decay** (excluyendo par√°metros de BatchNorm):

  L2: $g_t \leftarrow g_t + \lambda \theta_t$
  
  L1: $g_t \leftarrow g_t + \lambda \cdot \text{sign}(\theta_t)$

#### `rmsprop.py`

- **Prop√≥sito**: Implementa el optimizador RMSprop que mantiene un promedio m√≥vil de gradientes al cuadrado para adaptar la tasa de aprendizaje por par√°metro
- **Clase principal**: `RMSprop`
- **Caracter√≠sticas principales**:
  - Promedio m√≥vil de gradientes al cuadrado para adaptar el tama√±o de paso por par√°metro
  - Soporte para weight decay L1 y L2
  - Exclusi√≥n autom√°tica de par√°metros de BatchNorm del weight decay
  - Coeficiente de decaimiento configurable para el promedio m√≥vil
  - Implementaci√≥n simple y eficiente
- **Integraci√≥n**:
  - Opera sobre listas de `Parameters` de `novann.module`
  - Usa el tipo `ListOfParameters` de `novann._typing`
  - Reconocimiento autom√°tico de par√°metros de BatchNorm por nombre (`gamma`, `beta`)
  - Compatible con la interfaz est√°ndar de optimizadores del framework
- **Uso en el framework**:
  - Alternativa a Adam para problemas donde se prefieren adaptaciones m√°s conservadoras
  - Utilizable en redes recurrentes y otros contextos donde RMSprop ha demostrado buen desempe√±o
  - Opci√≥n disponible en los ejemplos de entrenamiento
- **Detalles t√©cnicos**:

  **Algoritmo de actualizaci√≥n**:

  Para cada par√°metro $\theta$:

  $$E[g^2]_t = \beta E[g^2]_{t-1} + (1 - \beta) g_t^2$$
  
  $$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_t$$

  donde:
  - $\eta$: Tasa de aprendizaje (`lr`)
  - $\beta$: Coeficiente de decaimiento para el promedio m√≥vil
  - $g_t$: Gradiente en el paso $t$
  - $\epsilon$: T√©rmino de estabilidad num√©rica

  **Weight decay**: Igual que en Adam, aplicado antes de la actualizaci√≥n del par√°metro.

#### `sgd.py`

- **Prop√≥sito**: Implementa el optimizador SGD (Stochastic Gradient Descent) con momentum, weight decay y gradient clipping global
- **Clase principal**: `SGD`
- **Caracter√≠sticas principales**:
  - Descenso de gradiente estoc√°stico cl√°sico con momentum opcional (Polyak momentum)
  - Gradient clipping global para prevenir explosi√≥n de gradientes
  - Soporte para weight decay L1 y L2
  - Exclusi√≥n autom√°tica de par√°metros de BatchNorm del weight decay
  - Implementaci√≥n eficiente con buffers de velocidad para momentum
- **Integraci√≥n**:
  - Opera sobre listas de `Parameters` de `novann.module`
  - Usa el tipo `ListOfParameters` de `novann._typing`
  - Sistema de gradient clipping que considera la norma total de todos los gradientes
  - Compatible con la interfaz de entrenamiento del framework
- **Uso en el framework**:
  - Optimizador est√°ndar para problemas donde se prefiere simplicidad y control fino
  - √ötil para fine-tuning y problemas con datos peque√±os
- **Detalles t√©cnicos**:

  **Algoritmo de actualizaci√≥n** (con momentum):

  Para cada par√°metro $\theta$:

  $$v_t = \beta v_{t-1} - \eta g_t$$
  
  $$\theta_{t+1} = \theta_t + v_t$$

  Sin momentum:

  $$\theta_{t+1} = \theta_t - \eta g_t$$

  **Gradient clipping global**:

  $$\text{total\_norm} = \sqrt{\sum_i \|g_i\|^2}$$
  
  $$\text{clip\_coef} = \min\left(1.0, \frac{\text{max\_grad\_norm}}{\text{total\_norm} + 1e-6}\right)$$
  
  $$g_i \leftarrow g_i \cdot \text{clip\_coef}$$

  **Weight decay**: Aplicado al gradiente antes de la actualizaci√≥n, excluyendo par√°metros de BatchNorm.

  **Caracter√≠sticas comunes de los optimizadores**:
  - Todos implementan `step()` para actualizar par√°metros y `zero_grad()` para limpiar gradientes
  - Excluyen par√°metros `gamma` y `beta` de BatchNorm del weight decay (detectados por nombre)
  - Manejan adecuadamente par√°metros sin gradiente (`grad is None`)
  - Son iterables sobre listas de par√°metros materializadas

### `üìÇ utils/`

**Utilidades para manejo de datos, carga de datasets, logging, visualizaciones, entrenamiento y verificaci√≥n de gradientes**

Contiene:
- `data/`: Utilidades para manejo de datos y preprocesamiento
- `datasets/`: Funciones para cargar datasets comunes de visi√≥n por computadora
- `decorators/`: Decoradores para timing y profiling
- `gradient_checking/`: Utilidades para verificaci√≥n num√©rica de gradientes
- `log_config/`: Configuraci√≥n de sistema de logging
- `train/`: Funci√≥n de entrenamiento de modelos
- `visualizations/`: Utilidades para visualizaci√≥n de resultados y m√©tricas

#### `üìÇ utils/üìÇ data/`

**Utilidades para manejo de datos y preprocesamiento**

Contiene:
- `dataloader.py`: Clase `DataLoader` para iterar sobre datasets en minibatches
- `preprocessing.py`: Funciones para normalizaci√≥n y separaci√≥n de caracter√≠sticas y etiquetas

##### `dataloader.py`

- **Prop√≥sito**: Implementa un DataLoader iterable que permite recorrer un dataset en minibatches, con soporte para shuffling
- **Clase principal**: `DataLoader`
- **Caracter√≠sticas principales**:
  - Soporta batches de tama√±o fijo configurable
  - Puede barajar los datos al inicio de cada √©poca
  - Implementa el protocolo iterador de Python con iterador interno `_Iter`
  - Calcula autom√°ticamente el n√∫mero de batches por √©poca mediante `__len__`
  - Maneja correctamente el √∫ltimo batch que puede ser m√°s peque√±o
- **Integraci√≥n**:
  - Utilizado en los scripts de ejemplos para proporcionar datos a los modelos durante entrenamiento y evaluaci√≥n
  - Compatible con las funciones de m√©tricas (`accuracy`, `binary_accuracy`, `r2_score`) que requieren un iterador de batches
  - Tipo `Loader` definido en `novann/_typing.py` se refiere a esta clase
- **Uso en el framework**:
  - Se utiliza en los ciclos de entrenamiento para obtener batches de datos
  - Tambi√©n se utiliza en la evaluaci√≥n para calcular m√©tricas sobre el dataset completo
  - Permite iteraci√≥n eficiente sobre datasets grandes sin cargarlos completamente en memoria

##### `preprocessing.py`

- **Prop√≥sito**: Funciones de preprocesamiento para normalizaci√≥n y separaci√≥n de caracter√≠sticas y etiquetas
- **Funciones principales**:
  - `normalize`: Normaliza los datos restando la media y dividiendo por la desviaci√≥n est√°ndar
  - `split_features_and_labels`: Separa un DataFrame de pandas en arrays de caracter√≠sticas y etiquetas
- **Implementaci√≥n**:
  
  **Normalizaci√≥n**:
  
  $$x_{\text{norm}} = \frac{x - \mu}{\sigma}$$
  
  **Separaci√≥n de caracter√≠sticas y etiquetas**:
  - Detecta autom√°ticamente si existe columna "label" en el DataFrame
  - Si no existe, asume que la primera columna son las etiquetas
- **Integraci√≥n**:
  - Utilizada por las funciones de carga de datasets (`load_fashion_mnist_data` y `load_mnist_data`)
  - La normalizaci√≥n utiliza estad√≠sticas del conjunto de entrenamiento para evitar data leakage
  - Compatible con DataFrames de pandas cargados desde CSV
- **Uso en el framework**:
  - Preprocesamiento de datos antes del entrenamiento
  - Normalizaci√≥n de caracter√≠sticas para estabilizar el entrenamiento
  - Separaci√≥n de datos en caracter√≠sticas (X) y etiquetas (y)

#### `üìÇ utils/üìÇ datasets/`

**Funciones para cargar datasets comunes de visi√≥n por computadora**

Contiene:
- `fashion.py`: Carga del dataset Fashion-MNIST
- `mnist.py`: Carga del dataset MNIST

##### `fashion.py`

- **Prop√≥sito**: Carga el dataset Fashion-MNIST desde archivos CSV y opcionalmente lo normaliza y transforma a tensores 4D
- **Funci√≥n principal**: `load_fashion_mnist_data`
- **Caracter√≠sticas principales**:
  - Carga los datos de entrenamiento, prueba y validaci√≥n desde rutas especificadas
  - Soporta normalizaci√≥n usando la media y desviaci√≥n est√°ndar del conjunto de entrenamiento
  - Puede convertir los datos a formato 4D `(N, 1, 28, 28)` para capas convolucionales 2D
  - Usa pandas con backend pyarrow para eficiencia en memoria
  - Manejo de errores con logging apropiado
  - Retorna tuplas `(x_train, y_train), (x_test, y_test), (x_val, y_val)`
- **Integraci√≥n**:
  - Utiliza `split_features_and_labels` y `normalize` de `utils/data/preprocessing.py`
  - Usa constantes de `core/constants.py` para las rutas por defecto (`EXPORTATION_FASHION_TRAIN_DATA_PATH`, etc.)
  - Retorna tuplas de tipo `TrainTestEvalSets` definido en `novann/_typing.py`
  - Registra eventos con el logger de `novann/utils/log_config/`
- **Uso en el framework**:
  - Proporciona los datos para los ejemplos de clasificaci√≥n de im√°genes
  - Utilizado en comparaciones con PyTorch y experimentos de benchmark

##### `mnist.py`

- **Prop√≥sito**: Carga el dataset MNIST desde archivos CSV y opcionalmente lo normaliza y transforma a tensores 4D
- **Funci√≥n principal**: `load_mnist_data`
- **Caracter√≠sticas principales**:
  - Funcionalidad similar a `load_fashion_mnist_data` pero para el dataset MNIST
  - Normalizaci√≥n opcional con estad√≠sticas del conjunto de entrenamiento
  - Transformaci√≥n a 4D `(N, 1, 28, 28)` para convoluciones 2D
  - Uso de pandas con pyarrow para carga eficiente
  - Manejo robusto de errores con logging
- **Integraci√≥n**:
  - Utiliza las mismas funciones de preprocesamiento que `fashion.py`
  - Usa constantes de `core/constants.py` para rutas MNIST (`EXPORTATION_MNIST_TRAIN_DATA_PATH`, etc.)
  - Mismo tipo de retorno `TrainTestEvalSets` y manejo de errores
- **Uso en el framework**:
  - Proporciona el dataset MNIST para ejemplos de clasificaci√≥n
  - Dataset cl√°sico para pruebas y demostraciones del framework


#### `üìÇ utils/üìÇ decorators/`

**Decoradores para funcionalidades transversales como timing y profiling**

Contiene:
- `timing.py`: Decorador `@chronometer` para medir tiempo de ejecuci√≥n de funciones

##### `timing.py`

- **Prop√≥sito**: Proporciona el decorador `@chronometer` para medir y registrar el tiempo de ejecuci√≥n de funciones de manera autom√°tica y no intrusiva
- **Decorador principal**: `@chronometer`
- **Caracter√≠sticas principales**:
  - Mide el tiempo de ejecuci√≥n con alta precisi√≥n usando `time.perf_counter()`
  - Formato inteligente del tiempo: adapta unidades desde nanosegundos hasta horas
  - Uso de emojis contextuales (‚ö° para r√°pido, ‚è±Ô∏è para normal, üê¢ para lento)
  - Preserva los metadatos de la funci√≥n original con `@wraps(func)`
  - No modifica el resultado de la funci√≥n decorada
  - Logging autom√°tico usando el sistema de logging del framework
- **Integraci√≥n**:
  - Importa y utiliza `logger` de `novann/utils/log_config/` para registrar tiempos
  - Decorador gen√©rico que puede aplicarse a cualquier funci√≥n callable
  - Utilizado por la funci√≥n `train()` de `utils/train/train.py` para medir tiempo de entrenamiento
- **Uso en el framework**:
  - Profiling de funciones cr√≠ticas para optimizaci√≥n de rendimiento
  - Medici√≥n de tiempo de entrenamiento en los ejemplos y experimentos
  - Debugging de rendimiento en desarrollo
- **Detalles t√©cnicos**:

  **Algoritmo de formato de tiempo**:
  
  - < 1 microsegundo: muestra en nanosegundos (ns)
  - < 1 milisegundo: muestra en microsegundos (Œºs) 
  - < 1 segundo: muestra en milisegundos (ms)
  - < 1 minuto: muestra en segundos con 2 decimales (s)
  - < 1 hora: muestra en minutos y segundos (m s)
  - ‚â• 1 hora: muestra en horas, minutos y segundos (h m s)

  **Implementaci√≥n**:
  ```python
  # Ejemplo de uso
  @chronometer
  def funcion_lenta():
      # c√≥digo que tarda
      pass
  ```

#### `üìÇ utils/üìÇ gradient_checking/`

**Utilidades para verificaci√≥n num√©rica de gradientes mediante diferencias finitas**

Contiene:
- `numerical.py`: Funciones para calcular gradientes num√©ricos usando diferencias centrales

##### `numerical.py`

- **Prop√≥sito**: Implementa funciones para calcular gradientes num√©ricos mediante diferencias finitas, utilizadas para verificar la correcci√≥n de las implementaciones de backpropagation
- **Funciones principales**:
  - `numeric_grad_elementwise`: Gradiente num√©rico elemento a elemento para funciones vectoriales
  - `numeric_grad_scalar_from_softmax`: Gradiente num√©rico espec√≠fico para softmax + p√©rdida escalar
  - `numeric_grad_scalar_wrt_x`: Gradiente num√©rico gen√©rico para p√©rdidas escalares
  - `numeric_grad_wrt_param`: Gradiente num√©rico respecto a par√°metros de capas
- **Caracter√≠sticas comunes**:
  - Todas usan diferencias centrales para mayor precisi√≥n: $\frac{f(x+\epsilon) - f(x-\epsilon)}{2\epsilon}$
  - Restauran los valores originales despu√©s de cada perturbaci√≥n
  - Iteran sobre elementos individuales usando `np.nditer` para manejar arrays multidimensionales
  - T√©rmino `eps` configurable para el tama√±o del paso de diferencias finitas
- **Integraci√≥n**:
  - Utilizadas en los tests unitarios para verificar gradientes de activaciones, capas lineales y convolucionales
  - Independientes de las implementaciones espec√≠ficas, trabajan con cualquier funci√≥n que siga la interfaz adecuada
  - No se utilizan durante el entrenamiento normal, solo para debugging y testing
- **Detalles t√©cnicos**:

  **Diferencias centrales** (f√≥rmula base):
  
  $$\frac{\partial f}{\partial x_i} \approx \frac{f(x_i + \epsilon) - f(x_i - \epsilon)}{2\epsilon}$$

  **`numeric_grad_elementwise`**:
  - Para funciones vectoriales $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$
  - Calcula $\frac{\partial f_j}{\partial x_i}$ para cada $i,j$
  - Usada para verificar gradientes de funciones de activaci√≥n

  **`numeric_grad_scalar_from_softmax`**:
  - Para $L = \sum(\text{softmax}(x) \cdot G)$ donde $G$ es una matriz de pesos
  - Calcula $\frac{\partial L}{\partial x_i}$
  - Espec√≠fica para testing de softmax + cross-entropy

  **`numeric_grad_scalar_wrt_x`**:
  - Para $S = \sum(\text{forward}(x) \cdot G)$
  - Calcula $\frac{\partial S}{\partial x_i}$
  - Versi√≥n gen√©rica para cualquier funci√≥n forward

  **`numeric_grad_wrt_param`**:
  - Para $S = \sum(\text{layer.forward}(x) \cdot G)$
  - Calcula $\frac{\partial S}{\partial p_i}$ donde $p_i$ son par√°metros de una capa
  - Perturba los datos del par√°metro (`p.data`) y restaura despu√©s

#### `üìÇ utils/üìÇ log_config/`

**Configuraci√≥n del sistema de logging para el framework**

Contiene:
- `logger.py`: Implementaci√≥n del logger personalizado `Logger` con patr√≥n singleton

##### `logger.py`

- **Prop√≥sito**: Proporciona un sistema de logging unificado para todo el framework con soporte para consola y archivo
- **Clase principal**: `Logger` (implementa patr√≥n singleton)
- **Caracter√≠sticas principales**:
  - Patr√≥n singleton: solo una instancia en toda la aplicaci√≥n
  - Soporte para m√∫ltiples niveles: DEBUG, INFO, WARNING, ERROR
  - Handlers para consola y archivo (configurable)
  - Formato personalizable con timestamp, nivel, nombre del logger y mensaje
  - M√©todos para logging con datos adicionales via `**kwargs`
  - Cambio din√°mico de nivel de logging
- **Integraci√≥n**:
  - Importado y utilizado por todos los m√≥dulos del framework que necesitan logging
  - Usa constantes de `core/constants.py` para configuraci√≥n por defecto (`LOG_FILE`, `LOGGER_DEFAULT_FORMAT`, `LOGGER_DATE_FORMAT`)
  - Instancia global `logger` creada al nivel del m√≥dulo para f√°cil acceso
- **Uso en el framework**:
  - Debugging durante desarrollo y testing
  - Registro de eventos durante entrenamiento (p√©rdida, m√©tricas, errores)
  - Seguimiento de inicializaci√≥n de par√°metros en `Sequential`
  - Manejo de errores en carga de datasets

#### `üìÇ utils/üìÇ train/`

**Funci√≥n de entrenamiento de modelos**

Contiene:
- `train.py`: Funci√≥n `train()` que implementa el ciclo completo de entrenamiento

##### `train.py`

- **Prop√≥sito**: Proporciona una funci√≥n de alto nivel para entrenar modelos de forma sencilla y configurable
- **Funci√≥n principal**: `train()` (decorada con `@chronometer` para medici√≥n de tiempo)
- **Caracter√≠sticas principales**:
  - Ciclo completo de entrenamiento con epochs y batches
  - Soporte para m√©tricas de validaci√≥n peri√≥dicas
  - Logging configurable de progreso
  - Manejo autom√°tico de modos train/eval del modelo
  - Integraci√≥n con cualquier optimizador y funci√≥n de p√©rdida del framework
  - Decorada con `@chronometer` para medir tiempo de ejecuci√≥n
- **Integraci√≥n**:
  - Utiliza `DataLoader` de `utils/data/` para iterar sobre datos
  - Espera un modelo `Sequential` de `model/nn.py`
  - Compatible con cualquier `Optimizer` (Adam, SGD, RMSprop) y `LossFunc` (CrossEntropyLoss, MSE, etc.)
  - Usa el `logger` de `utils/log_config/` para registro de progreso
  - Decorador `@chronometer` de `utils/decorators/` para timing
- **Uso en el framework**:
  - Funci√≥n principal para entrenar modelos en los ejemplos y experimentos
  - Simplifica el c√≥digo de entrenamiento eliminando la necesidad de escribir loops manuales
  - Proporciona un punto de entrada est√°ndar para el entrenamiento

## Patrones de Uso com√∫n

Digamos que queremos hacer un clasificador de imagenes para el dataset _fashion-mnist_ el flujo normal de trajo ser√≠a:

```python
# 1. importar las herramientas necesarias
from novann.model import Sequential
from novann.optim import Adam
from novann.utils.data import DataLoader
from novann.losses import CrossEntropyLoss
from novann.metrics import accuracy
from novann.utils.datasets import load_fashion_mnist_data
from novann.layers import (
    Conv2d,
    Linear, 
    ReLU,
    Flatten
    BatchNorm2d, 
    MaxPool2d
)

# 2. cargar los datos a utilizar
(x_train, y_train), (x_test, y_test), (x_val, y_val) = load_fashion_mnist_data(
    tensor4d=True, do_normalize=True
)

# 3. definir el modelo
model = Sequential(
    Conv2d(1, 32, 3, padding=1, bias=False),
    BatchNorm2d(32),
    ReLU(),
    MaxPool2d(2, 2),
    Conv2d(32, 64, 3, padding=1, bias=False),
    BatchNorm2d(64),
    ReLU(),
    MaxPool2d(2, 2),
    Linear(64 * 8 * 8, 10) # -> 10 clases (del 0 al 9)
)

# si imprime el modelo vera algo como
print(model)
"""
Sequential(
  (0): Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (1): BatchNorm2d(num_features=32, momentum=0.1, eps=1e-05)
  (2): ReLU()
  (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))
  (4): Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (5): BatchNorm2d(num_features=64, momentum=0.1, eps=1e-05)
  (6): ReLU()
  (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))
  (8): Linear(in_features=4096, out_features=10, bias=True)
)
"""

# 4. Establecer optimizador e hiperparametros
lr = 1e-3
batch_size = 128
epochs = 10
optimizer = Adam(
    model.parameters() # Se le pasan los parametros del modelo
    lr=lr, 
    weight_decay=1e-5
    betas=(0.9,0.999)
)

# 5. definimos la funcion de perdida
loss_fn = CrossEntropyLoss()

# 6. Crear los data loaders
train_loader = DataLoader(x_train, y_train, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(x_val, y_val, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(x_test, y_test, batch_size=batch_size, shuffle=False)

# 7. hacer un loop de entrenamiento (o llamar la funci√≥n train)

model.train() # poner en modo entrenamiento

for epoch in range(epochs):
    for input, label in train_loader:
        # poner gradientes a zero
        optimizer.zero_grad()

        # caluclar forward pass
        logits = model(input)

        # calcular la perdida total del mini batch y el gradiente
        loss, grad = loss_fn(logits, label)

        # ejecutar el backward pass
        model.backward(grad)

        # actualizar los parametros una vez calculado los gradientes
        optimizer.step()
    
    model.eval() # poner en modo evaluaci√≥n
    acc = accuracy(model, val_loader)

    model.train() # Establecer de nuevo en modo entrenamiento

    # (Opcional) imprimir resultados
    print(f"Epoch {epoch + 1}/{epochs}, loss: {loss:.4f}, validation accuracy: {acc:.3f}")

# 8. evaluaci√≥n final con el set de prueba
model.eval()
acc = accuracy(model, test_loader)
print(f"Test accuracy {acc:.3}")
```

## üõ†Ô∏è Tecnolog√≠as utilizadas

El framework **NovaNN** est√° construido utilizando las siguientes tecnolog√≠as y librer√≠as principales:

- **Lenguaje**: Python >= 3.14 
- **Gesti√≥n de dependencias**: Poetry (para manejo de paquetes y entornos virtuales)
- **Librer√≠as principales**:
  - `numpy`: Operaciones num√©ricas eficientes y arrays multidimensionales
  - `pandas`: Manejo y an√°lisis de datos tabulares (para carga de datasets)
  - `matplotlib`: Visualizaci√≥n de gr√°ficos y resultados
  - `seaborn`: Mejora est√©tica de visualizaciones estad√≠sticas
  - `scikit-learn`: Herramientas de Machine Learning cl√°sico y utilidades
  - `pyarrow`: Backend eficiente para DataFrames de pandas (reduce uso de memoria)
- **Herramientas de desarrollo**:
  - `pytest`: Framework de testing unitario
  - `pytest-cov`: Cobertura de c√≥digo en tests
  - `python-dotenv`: Manejo de variables de entorno desde archivos `.env`
  - `ipykernel`: Kernel de Jupyter para notebooks
  - `black`: Formateador de c√≥digo para mantener estilo consistente

## üì¶ Instalaci√≥n

NovaNN utiliza **Poetry** para la gesti√≥n de dependencias y empaquetado. Sigue estos pasos para configurar el entorno:

### 1. Clonar el repositorio

```bash
git clone git@github.com:JOSE-MDG/NovaNN.git
cd NovaNN
```

### 2. Instalar Poetry (si no lo tienes instalado)
- Windows (PowerShell):

```bash
(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -
``` 

- Linux/macOS:

```bash
# Con curl
curl -sSL https://install.python-poetry.org | python3 -

# Con pipx
pipx install poetry
```

- A√±adir Poetry al PATH (si es necesario):

```bash
# En Linux/macOS, a√±adir al ~/.bashrc o ~/.zshrc:
export PATH="$HOME/.local/bin:$PATH"
```

### 3. Instalar dependencias del proyecto

```bash
# Instalar todas las dependencias (incluyendo las de desarrollo)
poetry install
```

### 4. Activar el entorno virtual

```bash
# instalaer el plugin de shell
poetry self add poetry-plugin-shell

# # Activar el shell con el entorno virtual
poetry shell

# Alternativamente, ejecutar comandos directamente sin activar el shell:
poetry run python examples/binary_classification.py
```

### 5. Configurar variables de entorno

Crea un archivo .env en la ra√≠z del proyecto con las siguientes variables (ajusta las rutas seg√∫n tu configuraci√≥n):

```env
# Rutas para Fashion-MNIST
FASHION_TRAIN_DATA_PATH=<SU RUTA>/NovaNN/data/FashionMnist/fashion-mnist_train.csv
EXPORTATION_FASHION_TRAIN_DATA_PATH=<SU RUTA>/data/FashionMnist/fashion_train_ready.csv
FASHION_VALIDATION_DATA_PATH=<SU RUTA>/data/FashionMnist/fashion_validation_ready.csv
FASHION_TEST_DATA_PATH=<SU RUTA>/data/FashionMnist/fashion-mnist_test.csv

# Rutas para MNIST
MNIST_TRAIN_DATA_PATH=<SU RUTA>/data/Mnist/mnist_train.csv
EXPORTATION_MNIST_TRAIN_DATA_PATH=<SU RUTA>/data/Mnist/mnist_train_ready.csv
MNIST_VALIDATION_DATA_PATH=<SU RUTA>/data/Mnist/mnist_validation_ready.csv
MNIST_TEST_DATA_PATH=<SU RUTA>/data/Mnist/mnist_test.csv

# Configuraci√≥n de logging
LOG_FILE=<SU RUTA>/logs/nova_nn.log
LOGGER_DEFAULT_FORMAT=%(asctime)s | %(levelname)-8s | %(name)s:%(funcName)s - %(message)s # Puede ser el que usted quiera
LOGGER_DATE_FORMAT=%Y-%m-%d %H:%M:%S
```

### 6. Ejecutar ejemplos

```bash
# Clasificaci√≥n binaria
poetry run python examples/binary_classification.py

# Clasificaci√≥n multiclase
poetry run python examples/multiclass_classification.py

# Redes convolucionales
poetry run python examples/conv_example.py

# Regresi√≥n
poetry run python examples/regresion.py
```

### 7. Ejecutar todos los tests

```bash
# Todos los tests
poetry run pytest tests/

# Tests espec√≠ficos con cobertura
poetry run pytest tests/ --cov=novann --cov-report=term-missing

# Tests verbosos
poetry run pytest tests/ -v
```

## üß™ Testing
El framework incluye una suite completa de tests unitarios en el directorio [`tests/`](./tests/) que verifican la correcta implementaci√≥n de todos los componentes. Para m√°s informaci√≥n vaya a [Tests unitarios](./tests/README.md)

## ü§ù Contribuci√≥n

Las contribuciones son bienvenidas y apreciadas. NovaNN es un proyecto educativo de c√≥digo abierto que se beneficia de la comunidad.

### **¬øC√≥mo Contribuir?**

1. **Fork del repositorio** en GitHub
2. **Crea una rama para tu feature** (`git checkout -b feature/nueva-funcionalidad`)
3. **Commit de tus cambios** (`git commit -m 'A√±ade nueva funcionalidad X'`)
4. **Push a la rama** (`git push origin feature/nueva-funcionalidad`)
5. **Abre un Pull Request** en GitHub con una descripci√≥n clara de los cambios

### **√Åreas de Contribuci√≥n Prioritarias**

- üêõ **Reporte y correcci√≥n de bugs**: Probar el framework en diferentes escenarios
- üí° **Nuevas capas y funcionalidades**: Implementaciones de papers recientes
- üìö **Mejora de documentaci√≥n**: Ejemplos adicionales, tutoriales, documentaci√≥n de c√≥digo
- üß™ **Tests unitarios**: Aumentar cobertura y casos edge
- ‚ö° **Optimizaciones de rendimiento**: Mejoras en implementaciones NumPy
- üîß **Herramientas de desarrollo**: Scripts de utilidad, visualizaciones

### **Gu√≠as de Estilo y Calidad**

- **C√≥digo**: Sigue las convenciones existentes y usa Black para formateo
- **Tests**: Incluye tests unitarios para nuevas funcionalidades
- **Documentaci√≥n**: Actualiza docstrings y README si es necesario
- **Tipado**: Usa type hints consistentemente
- **Commits**: Mensajes descriptivos en ingl√©s o espa√±ol

### **Proceso de Revisi√≥n**

- Los PRs ser√°n revisados por el mantenedor principal
- Se esperan tests que pasen y cobertura mantenida
-  
- Se puede solicitar cambios antes de mergear

### **Reporte de Issues**

Al reportar un bug o solicitar una feature:

- **T√≠tulo claro y descriptivo**
- **Descripci√≥n detallada** del problema o solicitud
- **Pasos para reproducir** (para bugs)
- **Comportamiento esperado vs actual**
- **Entorno**: Versi√≥n de Python, sistema operativo, versi√≥n de NovaNN
- **C√≥digo de ejemplo** m√≠nimo para reproducir
- **Logs relevantes** (usar el logger del framework)

## üìÑ Licencia

Este proyecto est√° bajo la **Licencia MIT**. Ver el archivo [LICENCE](./LICENCE) para m√°s detalles.

**Resumen de la licencia MIT:**
- Software libre para usar, copiar, modificar, fusionar, publicar, distribuir
- Se puede usar para fines comerciales
- La licencia incluye derechos de autor originales
- No hay garant√≠a y los autores no son responsables de da√±os

## üë§ Autor y Mantenedor

**Juan Jos√©** - Developer & Machine Learning Engineer (16 a√±os)

- GitHub: [https://github.com/JOSE-MDG](https://github.com/JOSE-MDG)
- Email: josepemlengineer@gmail.com

**Sobre m√≠**: Con solo 16 a√±os, constru√≠ **NovaNN** desde cero como un proyecto educativo para demostrar mi pasi√≥n y comprensi√≥n profunda del deep learning. Este framework representa meses de estudio autodidacta, experimentaci√≥n y dedicaci√≥n, implementando cada algoritmo matem√°ticamente desde los papers originales.

**Agradecimientos:**
- Inspirado en PyTorch y otros frameworks de deep learning
- Comunidad de open source por herramientas y conocimientos compartidos
- Papers de investigaci√≥n que fundamentan las implementaciones

