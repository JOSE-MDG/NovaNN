```
‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó
‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë
‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë
‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë
‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë
‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù
                NovaNN ‚Äî Deep Learning Framework
```

![version](https://img.shields.io/badge/version-1.0.0-blue)
![python](https://img.shields.io/badge/python-3.14-brightgreen)
![license](https://img.shields.io/badge/license-MIT-blue)
![tests](https://img.shields.io/badge/tests-pytest-orange)

## üåê Idiomas disponibles

- üá¨üáß [English](README.en.md)
- üá™üá∏ [Espa√±ol](README.md)

Este mini framework ofrece herramientas y ejemplos para la creaci√≥n de redes neuronales **MLP** junto con m√≥dulos que brindan soporte y mejoran el entrenamiento de la red. Este proyecto intenta reflejar una buena comprensi√≥n y dominio sobre c√≥mo funcionan estas redes, inspirado en c√≥mo lo hacen los frameworks de deep learning m√°s populares como **PyTorch** y **TensorFlow**, especialmente **PyTorch** que fue la base en la que se inspir√≥ este proyecto.

**Aclaraci√≥n**: Este mini framework busca demostrar s√≥lidas bases y conocimientos sobre c√≥mo funcionan las redes neuronales, Deep Learning, Machine Learning, matem√°ticas, ingenier√≠a de software, buenas pr√°cticas, tests unitarios, dise√±o modular y preprocesamiento de datos.

## Introducci√≥n

- Este proyecto tiene una estructura completamente **modular**; incluye un directorio llamado `examples/` con ejemplos de **clasificaci√≥n binaria**, **clasificaci√≥n multiclase** y **regresi√≥n** de c√≥mo se pueden utilizar las herramientas que posee este mini framework.

- El directorio `data/` posee datasets como _Fashion-MNIST_ y _MNIST_ donde _Fashion-MNIST_ fue utilizado para comparar el rendimiento del proyecto con otro framework y _MNIST_ para realizar un ejemplo de uso normal de clasificaci√≥n en el directorio `examples/`.

- Se realiz√≥ una revisi√≥n, preprocesamiento y divisi√≥n previa de datos en `notebooks/exploration.ipynb` donde se visualizaron los datasets y se particion√≥ en ambos el set de validaci√≥n.

- El m√≥dulo `src/` es el m√≥dulo principal que contiene todas las partes y/o herramientas que conforman este mini framework. Este posee una estructura centralizada en donde `core/config.py` almacena y carga los valores de las variables de entorno para que puedan ser asequibles por el resto de m√≥dulos, y as√≠ no tener que cargarlas en cada script.

- Se evalu√≥ el rendimiento de **NovaNN Framework** con el popular framework de deep learning **PyTorch** en una tarea de clasificaci√≥n con el dataset de _Fashion-MNIST_, en la cual se utiliz√≥ exactamente el mismo dataset e hiperpar√°metros para ambas pruebas. Para obtener los resultados del cotejo se guard√≥ en formato `json` m√©tricas como el accuracy y la p√©rdida.

  - **[main.py](main.py)**: Este archivo implementa el c√≥digo de entrenamiento y la estructura de la red que se va a utilizar para el cotejo.
  - **[pytorch_comparison](https://colab.research.google.com/drive/1APfspox9ONmDWL0jFXmndHZ70UPjr9Mn?usp=sharing)**: En el notebook est√° el c√≥digo de entrenamiento versi√≥n para PyTorch, que realiza el mismo procedimiento que el script.

### Resultados del cotejo:

Una vez obtenidos los resultados, se hizo un script ([visualization.py](visualization.py)) para graficar los resultados de una manera m√°s presentable.

![comparison](images/comparison.png)

## üìÇ Estructura del proyecto

[Structure file](FileTree_NeuralNetwork.md)

```
üìÅ Neural Networks
‚îú‚îÄ‚îÄ üìÅ data
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ FashionMnist
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ Mnist
‚îú‚îÄ‚îÄ üìÅ examples
‚îÇ   ‚îú‚îÄ‚îÄ üêç binary_classification.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç multiclass_classification.py
‚îÇ   ‚îî‚îÄ‚îÄ üêç regresion.py
‚îú‚îÄ‚îÄ üìÅ images
‚îÇ   ‚îî‚îÄ‚îÄ üñºÔ∏è comparison.png
‚îú‚îÄ‚îÄ üìÅ logs
‚îú‚îÄ‚îÄ üìÅ notebooks
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ exploration.ipynb
‚îú‚îÄ‚îÄ üìÅ src
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ core
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç config.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç dataloader.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç init.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç logger.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ layers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ activations
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç activations.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç relu.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç sigmoid.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç softmax.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç tanh.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ bn
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç batch_normalization.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ linear
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç linear.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ regularization
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç dropout.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ losses
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç functional.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ metrics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç nn.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ module
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç layer.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç module.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ optim
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç adam.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç rmsprop.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç sgd.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ üêç utils.py
‚îú‚îÄ‚îÄ üìÅ tests
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ activations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_leaky_relu.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_relu.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_sigmoid.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üêç test_softmax.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_tanh.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ batch_norm
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_batch_norm.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ dataloader
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_dataloader.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ initializers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üêç test_init.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç test_dropout_regularization.py
‚îÇ   ‚îú‚îÄ‚îÄ üêç test_linear_layer.py
‚îÇ   ‚îî‚îÄ‚îÄ üêç test_sequential_module.py
‚îú‚îÄ‚îÄ ‚öôÔ∏è .gitignore
‚îú‚îÄ‚îÄ üìù FileTree_NeuralNetwork.md
‚îú‚îÄ‚îÄ üìù README.en.md
‚îú‚îÄ‚îÄ üìù README.es.md
‚îú‚îÄ‚îÄ üêç main.py
‚îú‚îÄ‚îÄ üìÑ requirements.txt
‚îî‚îÄ‚îÄ üêç visualization.py
```

## Estructura del modulo `src/` y sub directorios

Aqu√≠ que se va a explicar a detalle que hace cada submodulo y sus archivos

### `core/`

**Centraliza las funciones esenciales del proyecto**

#### `config.py`

- **Prop√≥sito**: Configuraciones globales y variables de entorno
- **Variables de entorno**:
  - Rutas de datasets: `FASHION_TRAIN_DATA_PATH`, `MNIST_TRAIN_DATA_PATH`, etc.
  - Configuraci√≥n de logging: `LOG_FILE`, `LOGGER_DEFAULT_FORMAT`, etc.
- **Diccionarios de inicializaci√≥n**:
  - `DEFAULT_NORMAL_INIT_MAP`: Funciones de inicializaci√≥n con distribuci√≥n normal (Xavier/Kaiming)
  - `DEFAULT_UNIFORM_INIT_MAP`: Funciones de inicializaci√≥n con distribuci√≥n uniforme (Xavier/Kaiming)
  - **Claves**: Nombres de activaciones (`relu`, `leakyrelu`, `tanh`, `sigmoid`, `default`)
- **Integraci√≥n con activaciones**: Las claves en los mapas de inicializaci√≥n corresponden a los nombres
  de las activaciones en `layers/activations/`
- **Uso en capas lineales**: Los mapas de inicializaci√≥n son utilizados por `Linear.reset_parameters()`
- **Integraci√≥n completa**: Los mapas de inicializaci√≥n alimentan `Linear.reset_parameters()` que crea `Parameters` objects
- **Ecosistema completo**: Los mapas de inicializaci√≥n alimentan el sistema de herencia Module ‚Üí Layer ‚Üí Linear
- **Uso en Sequential**: Los mapas de inicializaci√≥n son utilizados por `Sequential._aply_initializer_for_linear_layers()` para inicializaci√≥n autom√°tica

#### `dataloader.py`

- **Clase `DataLoader`**:
  - **Prop√≥sito**: Cargador de datos iterable para mini-lotes
  - **Caracter√≠sticas**:
    - Soporta barajado (shuffle) de datos
    - Tama√±o de lote configurable (`batch_size`)
    - Implementa el protocolo iterador Python
  - **Integraci√≥n completa**: Utilizado en todos los ejemplos para gesti√≥n eficiente de batches
  - **M√©todos principales**:
    - `__init__`: Inicializa con arrays `x` (features) e `y` (labels)
    - `__iter__`: Crea iterador para una √©poca
    - `__len__`: Retorna n√∫mero de lotes por √©poca

#### `init.py`

- **Prop√≥sito**: Utilidades para inicializaci√≥n de pesos
- **Uso en capas**: Las funciones de inicializaci√≥n son utilizadas por capas lineales bas√°ndose en la activaci√≥n siguiente
- **Funciones**:
  - `calculate_gain(nonlinearity, param)`: Calcula ganancia para una no linealidad
  - `xavier_normal_(shape, gain)`: Inicializaci√≥n Xavier normal
  - `xavier_uniform_(shape, gain)`: Inicializaci√≥n Xavier uniforme
  - `kaiming_normal_(shape, a, nonlinearity, mode)`: Inicializaci√≥n Kaiming normal
  - `kaiming_uniform_(shape, a, nonlinearity, mode)`: Inicializaci√≥n Kaiming uniforme
  - `random_init_(shape, gain)`: Inicializaci√≥n aleatoria peque√±a (default conservador)

#### `logger.py`

- **Clase `Logger`**:
  - **Prop√≥sito**: Logger personalizado con m√∫ltiples niveles
  - **Caracter√≠sticas**:
    - Logging en consola y/o archivo
    - Formateo personalizable
    - Soporte para datos adicionales via `**kwargs`
  - **M√©todos**:
    - `info`, `debug`, `warning`, `error`: Logging en diferentes niveles
    - `_create_console_handler`, `_create_file_handler`: Configuran handlers
- **Instancia por defecto**: `logger` para uso global en el proyecto

### `layers/activations/`

- **Base unificada**: Todas las activaciones heredan de `Activation` que a su vez hereda de `Layer` y `Module`

#### `activations.py`

- **Clase `Activation`**:
  - **Herencia**: Subclase de `Layer`
  - **Prop√≥sito**: Clase base para todas las capas de activaci√≥n
  - **Atributos**:
    - `name`: Nombre en min√∫sculas de la clase (identificador para mapas de inicializaci√≥n)
    - `affect_init`: Booleano que indica si la activaci√≥n influye en la inicializaci√≥n de pesos
  - **M√©todos**:
    - `get_init_key()`: Retorna la clave de inicializaci√≥n si `affect_init = True`
    - `init_key`: Propiedad que alias de `get_init_key()`

#### `relu.py`

- **Clase `ReLU`**:

  - **Herencia**: Subclase de `Activation`
  - **Configuraci√≥n**: `affect_init = True`
  - **Atributos**:
    - `_mask`: M√°scara booleana guardada durante el forward
  - **Forward**: Aplica la funci√≥n ReLU: $max(0, x)$ elemento a elemento
  - **Backward**: Retropropaga el gradiente usando la m√°scara: `grad * _mask`

- **Clase `LeakyReLU`**:
  - **Herencia**: Subclase de `Activation`
  - **Par√°metros**:
    - `negative_slope` (por defecto 0.01): Pendiente para valores negativos
  - **Atributos**:
    - `a`: Almacena el valor de `negative_slope`
    - `activation_param`: mismo valor que `a` (para consistencia)
    - `_cache_input`: Entrada guardada para backward
  - **Forward**: Aplica `x si x >= 0, sino Œ± * x`
  - **Backward**: Calcula `grad * np.where(x >= 0, 1.0, self.a)`

#### `sigmoid.py`

- **Clase `Sigmoid`**:
  - **Herencia**: Subclase de `Activation`
  - **Configuraci√≥n**: `affect_init = True`
  - **Atributos**:
    - `out`: Salida guardada del forward para usar en backward
  - **Forward**: Calcula la funci√≥n sigmoide: $\sigma(x) = \frac{1}{1 + e^{-x}}$
  - **Backward**: Calcula el gradiente: $\frac{\partial L}{\partial a} \cdot \sigma(x) \cdot (1 - \sigma(x))$

#### `softmax.py`

- **Clase `SoftMax`**:
  - **Herencia**: Subclase de `Activation`
  - **Configuraci√≥n**: `affect_init = False` (no afecta inicializaci√≥n)
  - **Integraci√≥n con p√©rdidas**: Dise√±ada para usarse con `CrossEntropyLoss` (afect_init=False)
  - **Par√°metros**:
    - `axis` (por defecto 1): Eje sobre el cual aplicar softmax
  - **Atributos**:
    - `out`: Salida guardada del forward
  - **Forward**: Calcula softmax num√©ricamente estable:
    $$S(z_i) = \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}$$
  - **Backward**: Calcula el producto Jacobiano-vector:
    $$\frac{\partial S_i}{\partial z_j} = S_i(\delta_{ij} - S_j)$$

#### `tanh.py`

- **Clase `Tanh`**:
  - **Herencia**: Subclase de `Activation`
  - **Configuraci√≥n**: `affect_init = True`
  - **Atributos**:
    - `out`: Salida guardada del forward para usar en backward
  - **Forward**: Calcula la funci√≥n tangente hiperb√≥lica: $\tanh(x)$
  - **Backward**: Calcula el gradiente: $\frac{\partial L}{\partial a} \cdot (1 - \tanh^2(x))$

### `layers/bn/`

#### `batch_normalization.py`

- **Clase `BatchNormalization`**:

  - **Algoritmo**: Batch Normalization (Ioffe & Szegedy, 2015)
  - **F√≥rmulas del Forward Pass (Modo Entrenamiento)**:

    **Estad√≠sticas del minibatch**:
    $$\mu = \frac{1}{m} \sum_{i=1}^{m} x_i$$
    $$\sigma^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu)^2$$

    **Normalizaci√≥n**:
    $$\hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}}$$

    **Escala y desplazamiento**:
    $$y_i = \gamma \hat{x}_i + \beta$$

    **Actualizaci√≥n de estad√≠sticas m√≥viles**:
    running_mean = (1 - momentum) _ running_mean + momentum _ Œº
    running_var = (1 - momentum) _ running_var + momentum _ œÉ¬≤

    **F√≥rmulas del Backward Pass**:

    **Gradientes respecto a par√°metros**:
    ‚àÇL/‚àÇŒ≥ = Œ£[i=1 to m] (‚àÇL/‚àÇy_i \* xÃÇ_i)
    ‚àÇL/‚àÇŒ≤ = Œ£[i=1 to m] ‚àÇL/‚àÇy_i

    **Gradiente respecto a la entrada** (versi√≥n vectorizada eficiente):
    ‚àÇL/‚àÇx_i = (Œ≥ / (m _ ‚àö(œÉ¬≤ + Œµ))) _ (m _ ‚àÇL/‚àÇxÃÇ_i - Œ£[j=1 to m] ‚àÇL/‚àÇxÃÇ_j - xÃÇ_i _ Œ£[j=1 to m] (‚àÇL/‚àÇxÃÇ_j \* xÃÇ_j))

    Donde:

    - ‚àÇL/‚àÇxÃÇ_i = ‚àÇL/‚àÇy_i \* Œ≥
    - m es el tama√±o del minibatch

  - **Modo Evaluaci√≥n**:
    $$\hat{x}_i = \frac{x_i - \text{running\_mean}}{\sqrt{\text{running\_var} + \epsilon}}$$
    $$y_i = \gamma \hat{x}_i + \beta$$

  - **Referencia**: Ioffe, S., & Szegedy, C. (2015). "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"

## **Detalles de Implementaci√≥n:**

### **Estabilidad Num√©rica**:

- **Varianza sin bias**: Usa $\frac{m}{m-1}$ para correcci√≥n de bias en el entrenamiento
- **√âpsilon**: Peque√±o t√©rmino $\epsilon$ para evitar divisi√≥n por cero

### **Cach√©s para Backward**:

- `x_hat`: Valores normalizados $\hat{x}_i$
- `mu`, `var`: Media y varianza del minibatch
- `x_mu`: Diferencias $x_i - \mu$

### **Propiedades Clave**:

- **Reducci√≥n de Internal Covariate Shift**: Estabiliza distribuci√≥n de entradas
- **Efecto regularizador**: Reduce dependencia de Dropout
- **Permite mayores learning rates**: Entrenamiento m√°s r√°pido y estable

### `layers/linear/`

#### `linear.py`

- **Clase `Linear`**:
  - **Herencia**: Subclase de `Layer`
  - **Prop√≥sito**: Capa lineal (completamente conectada) que realiza: $y = xW^T + b$
  - **Gesti√≥n de par√°metros**: Utiliza `Parameters` para `weight` y `bias`, permitiendo su entrenamiento por optimizadores
  - **Integraci√≥n con Sequential**: Dise√±ada para ser usada dentro de `Sequential` con inicializaci√≥n autom√°tica
  - **Par√°metros**:
    - `in_features`: N√∫mero de caracter√≠sticas de entrada
    - `out_features`: N√∫mero de caracter√≠sticas de salida
    - `bias`: Si incluye t√©rmino de sesgo (por defecto `True`)
    - `init`: Funci√≥n de inicializaci√≥n opcional
  - **Atributos**:
    - `weight`: Par√°metro de pesos con forma `(out_features, in_features)`
    - `bias`: Par√°metro de sesgo con forma `(1, out_features)` (opcional)
    - `_cache_input`: Entrada guardada para c√°lculo de gradientes
  - **M√©todos**:
    - `reset_parameters(initializer)`: (Re)inicializa pesos y sesgo
    - `forward(x)`: Calcula $x W^T + b$
    - `backward(grad)`: Calcula gradientes para pesos, sesgo y entrada
    - `parameters()`: Retorna lista de par√°metros entrenables

### `layers/regularization/`

#### `dropout.py`

- **Clase `Dropout`**:
  - **Herencia**: Subclase de `Layer`
  - **Prop√≥sito**: Regularizaci√≥n por apagado aleatorio de neuronas
  - **Par√°metros**:
    - `p`: Probabilidad de dropout (0.0 ‚â§ p < 1.0)
  - **Atributos**:
    - `_mask`: M√°scara binaria de elementos conservados
  - **Comportamiento**:
    - **Training**: Apaga neuronas aleatoriamente y escala las restantes
    - **Evaluation**: Pasa la entrada sin cambios
  - **M√©todos**:
    - `forward(x)`: Aplica dropout durante entrenamiento
    - `backward(grad)`: Propaga gradientes solo por neuronas activas

### `losses/`

#### `functional.py`

- **Clase `CrossEntropyLoss`**:

  - **Prop√≥sito**: P√©rdida de entrop√≠a cruzada para clasificaci√≥n multiclase
  - **Caracter√≠sticas**:
    - Espera `logits` de forma `(N, C)` y etiquetas como √≠ndices enteros `(N,)`
    - Usa softmax num√©ricamente estable internamente
    - Maneja tanto etiquetas one-hot como √≠ndices de clase
  - **Atributos**:
    - `y_hat`: Probabilidades softmax cacheadas
    - `y_one_hot`: Etiquetas codificadas one-hot
    - `eps`: T√©rmino de estabilidad num√©rica (1e-12)
  - **M√©todos**:
    - `forward(logits, targets)`: Calcula p√©rdida y cachea valores
    - `backward()`: Retorna gradiente respecto a logits: $\frac{\partial L}{\partial z} = \frac{\hat{y} - y}{N}$
    - `__call__()`: Conveniencia para obtener p√©rdida y gradiente juntos

- **Clase `MSE`**:

  - **Prop√≥sito**: Error cuadr√°tico medio para regresi√≥n
  - **Forward**: $\frac{1}{N} \sum (logits - targets)^2$
  - **Backward**: $\frac{2}{N} (logits - targets)$

- **Clase `MAE`**:

  - **Prop√≥sito**: Error absoluto medio para regresi√≥n
  - **Forward**: $\frac{1}{N} \sum |logits - targets|$
  - **Backward**: $\frac{sign(logits - targets)}{N}$

- **Clase `BinaryCrossEntropy`**:
  - **Prop√≥sito**: Entrop√≠a cruzada binaria para clasificaci√≥n binaria
  - **Espera**: Probabilidades (despu√©s de sigmoid) y etiquetas 0/1
  - **Forward**: $-\frac{1}{N} \sum [y \cdot \log(p) + (1-y) \cdot \log(1-p)]$
  - **Backward**: $\frac{p - y}{N}$

### `metrics/`

#### `metrics.py`

- **Funci√≥n `accuracy`**:

  - **Prop√≥sito**: Precisi√≥n para clasificaci√≥n multiclase
  - **Entrada**: Modelo que retorna logits/probabilidades y DataLoader
  - **C√°lculo**: $\frac{\text{predicciones correctas}}{\text{total muestras}}$
  - **Uso**: Para problemas como MNIST, Fashion-MNIST con 10 clases

- **Funci√≥n `binary_accuracy`**:

  - **Prop√≥sito**: Precisi√≥n para clasificaci√≥n binaria
  - **Entrada**: Modelo que retorna probabilidades [0,1] y DataLoader
  - **Umbral**: 0.5 para convertir probabilidades a clases 0/1
  - **Uso**: Para problemas binarios como make_moons

- **Funci√≥n `r2_score`**:
  - **Prop√≥sito**: Coeficiente de determinaci√≥n para regresi√≥n
  - **F√≥rmula**: $R^2 = 1 - \frac{SSE}{SST}$
  - **Interpretaci√≥n**:
    - 1.0: ajuste perfecto
    - 0.0: modelo igual que la media
    - < 0.0: modelo peor que la media

### `model/`

#### `nn.py`

- **Clase `Sequential`**:
  - **Herencia**: Subclase de `Layer`
  - **Prop√≥sito**: Contenedor secuencial para capas (similar a PyTorch)
  - **Caracter√≠sticas**:
    - Inicializaci√≥n autom√°tica de capas lineales basada en activaciones adyacentes
    - Gesti√≥n unificada de modos (train/eval) para todas las capas
    - Recopilaci√≥n autom√°tica de par√°metros
  - **Inicializaci√≥n inteligente**: `Sequential` autom√°ticamente selecciona inicializadores √≥ptimos bas√°ndose en las activaciones cercanas a cada capa lineal
  - **Integraci√≥n con `core/config.py`**: Usa `DEFAULT_NORMAL_INIT_MAP` para obtener funciones de inicializaci√≥n
  - **Soporte para LeakyReLU**: Detecta el par√°metro `a` y usa `kaiming_normal_` con el slope correcto
  - **M√©todos**:
    - `forward(x)`: Propagaci√≥n secuencial a trav√©s de todas las capas
    - `backward(grad)`: Retropropagaci√≥n en orden inverso
    - `parameters()`: Todos los par√°metros de todas las capas
    - `train()`/`eval()`: Configura modo para todas las capas

### `module/`

#### `module.py`

- **Clase `Parameters`**:

  - **Prop√≥sito**: Contenedor para tensores entrenables y sus gradientes
  - **Atributos**:
    - `data`: Valores actuales del par√°metro
    - `grad`: Gradientes acumulados (misma forma que `data`)
    - `name`: Identificador opcional para debugging
  - **M√©todos**:
    - `zero_grad(set_to_none)`: Reinicia gradientes a cero o `None`

- **Clase `Module`**:
  - **Prop√≥sito**: Clase base para todos los componentes de la red
  - **Atributos**:
    - `_training`: Flag que indica modo entrenamiento/evaluaci√≥n
  - **M√©todos**:
    - `train()`: Activa modo entrenamiento
    - `eval()`: Activa modo evaluaci√≥n
    - `parameters()`: Retorna par√°metros entrenables (vac√≠o por defecto)
    - `zero_grad()`: Reinicia gradientes de todos los par√°metros

#### `layer.py`

- **Clase `Layer`**:
  - **Herencia**: Subclase de `Module` y `ABC` (clase abstracta)
  - **Prop√≥sito**: Base abstracta para todas las capas de la red
  - **M√©todos abstractos**:
    - `forward(x)`: Transformaci√≥n de entrada a salida
    - `backward(grad)`: C√°lculo de gradientes respecto a la entrada

### `optim/`

#### `sgd.py`

- **Clase `SGD`**:

  - **Algoritmo**: Descenso de Gradiente Estoc√°stico con Momentum (Polyak, 1964)
  - **F√≥rmulas**:

    **Actualizaci√≥n de momentum**:
    $$v_t = \beta v_{t-1} - \eta \nabla J(\theta_t)$$

    **Actualizaci√≥n de par√°metros**:
    $$\theta_{t+1} = \theta_t + v_t$$

    **Con weight decay L2**:
    $$\theta_{t+1} = \theta_t + v_t - \eta \lambda \theta_t$$

    **Con weight decay L1**:
    $$\theta_{t+1} = \theta_t + v_t - \eta \lambda \text{sign}(\theta_t)$$

  - **Gradient Clipping** (opcional):
    $$\text{Si } \|g\| > c, \quad g = \frac{c \cdot g}{\|g\|}$$

#### `rmsprop.py`

- **Clase `RMSprop`**:

  - **Algoritmo**: RMSprop (Hinton, 2012) - no publicado formalmente
  - **F√≥rmulas**:

    **Media m√≥vil de gradientes al cuadrado**:
    $$E[g^2]_t = \beta E[g^2]_{t-1} + (1 - \beta) g_t^2$$

    **Actualizaci√≥n de par√°metros**:
    $$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_t$$

#### `adam.py`

- **Clase `Adam`**:

  - **Algoritmo**: Adam (Kingma & Ba, 2014) - "Adaptive Moment Estimation"
  - **F√≥rmulas**:

    **Momentos de primer y segundo orden**:
    $$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$
    $$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$

    **Correcci√≥n de bias**:
    $$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$
    $$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$

    **Actualizaci√≥n de par√°metros**:
    $$\theta_{t+1} = \theta_t - \frac{\eta \cdot \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

## **Caracter√≠sticas Comunes de los Optimizadores:**

### **Weight Decay**:

- **L2 Regularization**: $\theta \leftarrow \theta - \eta \lambda \theta$
- **L1 Regularization**: $\theta \leftarrow \theta - \eta \lambda \text{sign}(\theta)$
- **Exclusi√≥n**: Par√°metros `gamma`/`beta` de BatchNorm est√°n excluidos del weight decay

### **Gradient Clipping** (SGD):

- **Prop√≥sito**: Prevenir explosi√≥n de gradientes
- **F√≥rmula**: $\text{clip}(g, c) = \frac{c \cdot g}{\max(\|g\|, c)}$

### **Inicializaci√≥n**:

- Momentos (`velocities`, `moments`) inicializados a cero
- Contador de pasos (`t`) en Adam para correcci√≥n de bias

## üìö **Referencias de los Algoritmos:**

1. **SGD with Momentum**: Polyak, B. T. (1964). "Some methods of speeding up the convergence of iteration methods"
2. **RMSprop**: Hinton, G. (2012). Lecture 6e of "Neural Networks for Machine Learning"
3. **Adam**: Kingma, D. P., & Ba, J. (2014). "Adam: A Method for Stochastic Optimization"

### `utils.py`

**Prop√≥sito**: Utilidades para testing de gradientes y carga de datasets

#### **Funciones de Testing de Gradientes:**

- **`numeric_grad_elementwise(act_forward, x, eps=1e-6)`**:

  - **Prop√≥sito**: Gradiente num√©rico elemento a elemento para funciones vectoriales
  - **M√©todo**: Diferencias centrales
  - **F√≥rmula**:
    $$\frac{\partial f_i}{\partial x_j} \approx \frac{f_i(x_j + \epsilon) - f_i(x_j - \epsilon)}{2\epsilon}$$
  - **Uso**: Verificar gradientes de activaciones (ReLU, Sigmoid, Tanh)

- **`numeric_grad_scalar_from_softmax(softmax_forward, x, G, eps=1e-6)`**:

  - **Prop√≥sito**: Gradiente num√©rico para softmax + p√©rdida escalar
  - **Contexto**: $L = \sum(\text{softmax}(x) \cdot G)$
  - **F√≥rmula**:
    $$\frac{\partial L}{\partial x_j} \approx \frac{L(x_j + \epsilon) - L(x_j - \epsilon)}{2\epsilon}$$
  - **Uso**: Testing espec√≠fico para SoftMax + CrossEntropy

- **`numeric_grad_scalar_wrt_x(forward_fn, x, G, eps=1e-6)`**:

  - **Prop√≥sito**: Gradiente num√©rico gen√©rico para p√©rdidas escalares
  - **Contexto**: $S = \sum(\text{forward}(x) \cdot G)$
  - **F√≥rmula**: Similar a softmax pero para cualquier funci√≥n
  - **Uso**: Testing general de capas

- **`numeric_grad_wrt_param(layer, param_attr, x, G, eps=1e-6)`**:
  - **Prop√≥sito**: Gradiente num√©rico respecto a par√°metros de capas
  - **Contexto**: $S = \sum(\text{layer.forward}(x) \cdot G)$
  - **F√≥rmula**:
    $$\frac{\partial S}{\partial p_j} \approx \frac{S(p_j + \epsilon) - S(p_j - \epsilon)}{2\epsilon}$$
  - **Uso**: Verificar gradientes de pesos y biases en capas lineales

#### **Funciones de Carga de Datos:**

- **`normalize(x_data, x_mean, x_std)`**:

  - **Prop√≥sito**: Normalizaci√≥n est√°ndar de datos
  - **F√≥rmula**:
    $$x_{\text{norm}} = \frac{x - \mu}{\sigma}$$

- **`_split_features_and_labels(df)`**:

  - **Prop√≥sito**: Separar caracter√≠sticas y etiquetas de DataFrames
  - **L√≥gica**:
    - Si existe columna "label" ‚Üí usarla
    - Si no ‚Üí primera columna son etiquetas

- **`load_fashion_mnist_data(...)`**:

  - **Prop√≥sito**: Cargar dataset Fashion-MNIST
  - **Caracter√≠sticas**:
    - Normalizaci√≥n opcional con estad√≠sticas del training set
    - Usa PyArrow para eficiencia en memoria
    - Split autom√°tico en train/val/test

- **`load_mnist_data(...)`**:
  - **Prop√≥sito**: Cargar dataset MNIST cl√°sico
  - **Mismas caracter√≠sticas** que Fashion-MNIST

## üèóÔ∏è Arquitectura del Framework

El framework sigue una arquitectura modular inspirada en PyTorch con un sistema de herencia bien definido:

### **Sistema de Herencia Principal**

```
Module (base)
    ‚Ü≥ Layer (abstracta)
        ‚Ü≥ Linear, Activation, BatchNormalization, Dropout
            ‚Ü≥ Activation (base para activaciones)
                ‚Ü≥ ReLU, LeakyReLU, Sigmoid, SoftMax, Tanh
```

### **Flujo de Datos y Responsabilidades**

- **`Module`**: Gesti√≥n de estado (train/eval) y par√°metros entrenables
- **`Layer`**: Interfaz abstracta para transformaciones (forward/backward)
- **`Parameters`**: Contenedor para datos y gradientes de par√°metros
- **Capas Concretas**: Implementaciones espec√≠ficas de transformaciones
- **`Losses`**: Funciones de p√©rdida con el mismo patr√≥n forward/backward

### **Sistema de Inicializaci√≥n Inteligente**

El framework incluye inicializaci√≥n autom√°tica de par√°metros:

```python
# Sequential detecta autom√°ticamente las activaciones y elige inicializadores √≥ptimos
model = Sequential(
    Linear(784, 256),  # Inicializado con Kaiming (por ReLU siguiente)
    ReLU(),
    Linear(256, 10)    # Inicializado con Kaiming (por ReLU anterior)
)
```

### **Sistema de Optimizaci√≥n**

El framework implementa optimizadores modernos con sus f√≥rmulas originales:

- **SGD con Momentum** (Polyak, 1964): Aceleraci√≥n por inercia
- **RMSprop** (Hinton, 2012): Learning rate adaptativo por par√°metro
- **Adam** (Kingma & Ba, 2014): Combinaci√≥n de momentum y adaptaci√≥n

**Caracter√≠sticas avanzadas**:

- Weight decay (L1/L2) con exclusi√≥n autom√°tica de BatchNorm
- Gradient clipping para estabilidad num√©rica
- Correcci√≥n de bias en Adam (importante en primeras iteraciones)

### **Normalizaci√≥n por Lotes (BatchNorm)**

El framework implementa Batch Normalization seg√∫n el paper original de Ioffe y Szegedy (2015):

- **Estabilizaci√≥n del Entrenamiento**: Reduce el internal covariate shift
- **Aceleraci√≥n del Entrenamiento**: Permite usar learning rates m√°s altos
- **Regularizaci√≥n Ligera**: Reduce la necesidad de Dropout

**Caracter√≠sticas avanzadas**:

- **Modos distintos**: Comportamiento diferente en entrenamiento vs evaluaci√≥n
- **Estad√≠sticas m√≥viles**: Acumuladas durante entrenamiento para evaluaci√≥n
- **Par√°metros aprendibles**: `gamma` (escala) y `beta` (desplazamiento)
- **Exclusi√≥n inteligente**: Optimizadores excluyen `gamma`/`beta` del weight decay

### **Patr√≥n de Entrenamiento Unificado**

Basado en los ejemplos reales del proyecto, el flujo de trabajo t√≠pico es:

```python
# 1. CONFIGURACI√ìN DEL MODELO
model = Sequential(
    Linear(784, 256),
    BatchNormalization(256),
    ReLU(),
    Dropout(0.3),
    Linear(256, 10),
)

# 2. PREPARACI√ìN DE DATOS
train_loader = DataLoader(x_train, y_train, batch_size=256, shuffle=True)
val_loader = DataLoader(x_val, y_val, batch_size=256, shuffle=False)

# 3. CONFIGURACI√ìN DE ENTRENAMIENTO
model.train()  # Modo entrenamiento
optimizer = Adam(model.parameters(), learning_rate=1e-3, weight_decay=1e-4)
loss_fn = CrossEntropyLoss()

# 4. CICLO DE ENTRENAMIENTO
for epoch in range(epochs):
    for input, target in train_loader:
        # Resetear gradientes
        optimizer.zero_grad()

        # Forward pass
        output = model.forward(input)

        # Calcular p√©rdida
        loss, grad = loss_fn(output, target)

        # Backward pass
        model.backward(grad)

        # Actualizar par√°metros
        optimizer.step()

    # Validaci√≥n peri√≥dica
    model.eval()
    accuracy = accuracy(model, val_loader)
    model.train()
```

### **Patrones Espec√≠ficos por Tipo de Problema**

**Clasificaci√≥n Multiclase (MNIST, Fashion-MNIST)**

```python
# Arquitectura t√≠pica
Sequential(
    Linear(784, 512), BatchNormalization(512), ReLU(), Dropout(0.3),
    Linear(512, 256), BatchNormalization(256), ReLU(), Dropout(0.3),
    Linear(256, 10)  # Sin activaci√≥n - logits para CrossEntropy
)
loss_fn = CrossEntropyLoss()  # Incluye SoftMax internamente
```

**Clasificaci√≥n Binaria (Make Moons)**

```python
# Arquitectura t√≠pica
Sequential(
    Linear(2, 32), Tanh(), Dropout(0.2),
    Linear(32, 1), Sigmoid()  # Probabilidades para BinaryCrossEntropy
)
loss_fn = BinaryCrossEntropy()
```

**Regresi√≥n (Make Regression)**

```python
# Arquitectura t√≠pica
Sequential(
    Linear(45, 368), BatchNormalization(368), LeakyReLU(), Dropout(0.2),
    Linear(368, 176), BatchNormalization(176), LeakyReLU(), Dropout(0.4),
    Linear(176, 1)  # Salida continua
)
loss_fn = MSE()  # o MAE()
```

### **Componentes de evaluaci√≥n**

```python
# M√©tricas por tipo de problema
accuracy(model, data_loader)           # Clasificaci√≥n multiclase
binary_accuracy(model, data_loader)    # Clasificaci√≥n binaria
r2_score(model, data_loader)           # Regresi√≥n
```

## üõ†Ô∏è Tecnolog√≠as utilizadas

- Lenguajes: Python 3.14.0 üêç

- Herramientas de desarrollo: Estenciones: `Black Formatter`, `FileTree Pro`

- Principales Librerias utilizadas:

1.  **`numpy`**: Arrays con opreaciones vectorizadas optimizadas en memoria.
2.  **`pandas`**: Manejo de datos tabulares. Ej. Datasets como _fashion-mnist_ y _mnist_.
3.  **`matplotlib`**: Visualizaci√≥n de datos y graficos estad√≠sticos
4.  **`seaborn`**: Mejores estilos para las visualizaciones.
5.  **`scikit-learn`**: Libreria de Machine Learning clasico.
6.  **`pyarrow`**: Reducir el gran uso de memor√≠a de pandas con datasets grandes
7.  **`pytest`**: Tests unitarios.

## üì¶ Instalaci√≥n

Instrucciones para instalar dependecias y preparar el entrono

1. **Clonar repositorio**

```bash
git clone https://github.com/JOSE-MDG/NovaNN.git

# Acceder al directorio
cd NovaNN
```

2. **Crear y activar un entorno virtual**

- Windows:

```bash
python -m venv .venv

# Activar entorno virtual
.\\.venv\\Scripts\\activate

# Si el anterior da problemas (PowerShell)
.\\.venv\\Scripts\\Activate.Ps1
```

- Linux/MacOS:

```bash
python -m venv .venv

# Activar entorno virtual
source .venv/bin/activate
```

3. **Instalar los requerimientos (`requirements.txt `)**

```bash
pip install -r requirements.txt
```

## üß™ Testing (`tests/`)

El framework incluye una suite completa de tests unitarios que verifican la correcta implementaci√≥n de todos los componentes:

### **Tests de Activaciones** (`tests/activations/`)

#### `test_relu.py`

- **Verifica**: Forward pass (no negatividad) y backward pass (m√°scara de gradiente)
- **Propiedades**: Forma de salida, comportamiento en x>0 y x<0
- **Gradientes**: Comparaci√≥n anal√≠tica vs num√©rica para x ‚â† 0

#### `test_leaky_relu.py`

- **Verifica**: Comportamiento con pendiente negativa (`negative_slope`)
- **Propiedades**: Forward (x‚â•0 pasa, x<0 escala) y backward (1.0 vs slope)
- **Gradientes**: Validaci√≥n num√©rica para entradas no cero

#### `test_sigmoid.py`

- **Verifica**: Rango de salida (0,1) y c√°lculo de derivada
- **Propiedades**: Forma de salida y consistencia forward/backward
- **Gradientes**: Comparaci√≥n œÉ(x)\*(1-œÉ(x)) vs num√©rico

#### `test_softmax.py`

- **Verifica**: Estabilidad num√©rica y propiedades de probabilidad
- **Propiedades**: Suma a 1 por fila, invariancia a desplazamiento
- **Jacobiano**: Producto Jacobiano-vector vs aproximaci√≥n num√©rica

#### `test_tanh.py`

- **Verifica**: Rango de salida (-1,1) y propiedad de funci√≥n impar
- **Propiedades**: tanh(-x) = -tanh(x), derivada 1 - tanh¬≤(x)
- **Gradientes**: Validaci√≥n num√©rica de la derivada

### **Tests de BatchNorm** (`tests/batch_norm/`)

#### `test_batch_norm.py`

- **Verifica**: Comportamiento en modos training/evaluation
- **Training**: Normalizaci√≥n correcta (media ~0, varianza acotada)
- **Evaluation**: Uso de estad√≠sticas m√≥viles, salidas estables
- **Propiedades**: Conservaci√≥n de forma, centrado de caracter√≠sticas

### **Tests de DataLoader** (`tests/dataloader/`)

#### `test_dataloader.py`

- **Verifica**: Manejo correcto de batches, incluyendo el √∫ltimo batch m√°s peque√±o
- **Propiedades**: Conservaci√≥n de shapes, iteraci√≥n completa del dataset
- **Casos edge**: Batch_size que no divide exactamente el dataset

### **Tests de Initializers** (`tests/initializers/`)

#### `test_init.py`

- **Verifica**: Distribuciones estad√≠sticas de los inicializadores
- **Kaiming**: Media cero, varianza apropiada para ReLU/LeakyReLU
- **Xavier**: Escalamiento correcto para tanh/sigmoid
- **Validaci√≥n**: L√≠mites de distribuciones uniformes, manejo de errores
- **Excepciones**: Nonlinearities no soportadas levantan ValueError

### **Tests de Regularizaci√≥n**

#### `test_dropout_regularization.py`

- **Verifica**: Comportamiento en modos training vs evaluation
- **Training**: Aplicaci√≥n de m√°scara binaria y escalado correcto (1/(1-p))
- **Evaluation**: Paso-through sin cambios
- **Backward**: Gradientes masked y escalados id√©nticamente al forward

### **Tests de Capas Lineales**

#### `test_linear_layer.py`

- **Verifica**: Forward/backward de capas lineales con y sin bias
- **Shapes**: Conservaci√≥n de dimensiones de entrada/salida
- **Gradientes**: Validaci√≥n num√©rica de gradientes respecto a inputs y par√°metros
- **Par√°metros**: Gradientes de weight y bias vs aproximaciones num√©ricas

### **Tests de M√≥dulos Secuenciales**

#### `test_sequential_module.py`

- **Verifica**: Comportamiento de contenedores Sequential
- **Forward/Backward**: Propagaci√≥n a trav√©s de m√∫ltiples capas
- **Inicializaci√≥n**: Detecci√≥n autom√°tica de activaciones para inicializadores
- **Par√°metros**: Recopilaci√≥n correcta de todos los par√°metros entrenables
- **Shapes**: Conservaci√≥n de dimensions through the network

### **Metodolog√≠a de Testing**

- **Gradientes num√©ricos**: Usando funciones de `utils.py` para verificar backpropagation
- **Comparaci√≥n anal√≠tica**: `assert_allclose` con tolerancia 1e-5
- **Modos training/eval**: Verificaci√≥n de comportamientos diferentes
- **Coverage completo**: Todos los par√°metros entrenables y casos edge
- **RNG determin√≠stico**: Para tests reproducibles

### **Ejecuci√≥n de Tests**

```bash
# Ejecutar todos los tests
pytest tests/

# Ejecutar tests con output verboso
pytest tests/ -v

# Ejecutar tests espec√≠ficos
pytest tests/activations/test_relu.py
pytest tests/batch_norm/ -v
```

## ü§ù Contribuci√≥n

Las contribuciones son bienvenidas y apreciadas. Si deseas contribuir a NovaNN Framework, por favor sigue estos pasos:

### **¬øC√≥mo Contribuir?**

1. **Fork del proyecto**
2. **Crea una rama para tu feature** (`git checkout -b feature/AmazingFeature`)
3. **Commit de tus cambios** (`git commit -m 'Add some AmazingFeature'`)
4. **Push a la rama** (`git push origin feature/AmazingFeature`)
5. **Abre un Pull Request**

### **√Åreas de Contribuci√≥n**

- üêõ **Reporte de bugs** y issues
- üí° **Nuevas features** y mejoras
- üìö **Mejora de documentaci√≥n**
- üß™ **Tests adicionales**

### **Gu√≠as de Estilo**

- Sigue las convenciones de c√≥digo existentes
- Incluye tests para nuevas funcionalidades
- Actualiza la documentaci√≥n correspondiente

### **Reporte de Issues**

Al reportar un bug, por favor incluye:

- Versi√≥n de Python y dependencias
- Pasos para reproducir el problema
- Comportamiento esperado vs actual
- Logs relevantes si aplica

## üìÑ Licencia

Este proyecto est√° bajo la **Licencia MIT**. Ver el archivo [LICENSE](LICENCE) para m√°s detalles.

## üë§ Autor

Juan Jos√© - Developer, Machine & Deep Learning Enthusiast.
GitHub: https://github.com/JOSE-MDG
